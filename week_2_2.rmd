
---
title: "Cleaning the Observed and Thinking About the Unobserved Data"
output: html_document
bibliography: bibliography.bib
---


```{r include = FALSE}
library(tidyverse)
library(igraph)
library(knitr)
```


## Unharmonised and Missing data, or the Universals of Data Analysis

Although network analysis is the topic of this course, it shares many elements with other approaches to research that apply data, statistics and computation. Perhaps most universal of these similarities are the problems that need to be solved before (network) analysis can begin: those of unharmonised and missing data. In the  worst case, jumping over or failing the steps to tackle them can lead to completely faulty results or interpretations. Fortunately, there are things you can do to make your data (or, very least, your interpretation of it) better.

## Unharmonised Data - An Overview  

Computers do not have an understanding of relevant and irrelevant differences. This means, that even the smallest of inconsistencies in denoting a thing can have big differences in data analysis. These inconsistencies stem from various sources. There were often various ways to write a name of a city or a person during the early modern period, so often even data that has been "accurately" collected ends up being unharmonised. Both modern and pre-modern data are also subject to mistakes that happen when the data is recorded or transformed from one format to another. Regular users of research libraries will often find instances where mistakes have been made in the cataloging process, and the users of Twitter that make the contents of a social network data sets might not respect spelling rules to begin with. If these different variants of the same thing are not accounted for, the data will be inflated by pseudo persons, cities, books or other entities, that should be grouped together. 

```{r echo=FALSE,fig.cap="Some name variants of the City of Aachen according to CERL Thesaurus."}
aachen <- c("Aach","Achen", "Aix la Chapelle", "Aix-la-Chapelle")
print("Some name variants of the City of Aachen according to CERL Thesaurus:")
print(aachen)
```


This process of matching and standardising different ways of denoting the same things is called data harmonisation.
There are various ways of doing data harmonisation. In fact, it is a field of research on its own right [@Christen2012], and many algorithms and tools have been developed to help database developers and users to find and match instances of the same thing. Fortunately, it is not necessary to become an expert in all of these techniques to solve most data harmonisation tasks that you will encounter. Here, we introduce two entry-level tools, that will already solve significant number of harmonisation problems, and keep being useful even if you familiarise yourself with more advanced ones.

## Harmonisation Table  

The technically simplest solution is to construct a harmonisation table. The table has at least two columns, one for the unharmonised values and other for corresponding harmonised values. The number unharmonised and harmonised columns can also be greater. The pros of this approach are the technical ease and the possibility of doing such matching that would be hard to automate, like converting variations of [latin place names to moden standardised names](https://rbms.info/lpn/a/). You can then join the harmonised values of the variable to your table with the unharmonised values with the join-functions of R. However, this approach does not scale well and always requires manual labor. But for a small data set with intricacies of who maps to what (like is often the case with e.g. historical data) manual approach is sometimes the best choice.

```{r echo=FALSE,fig.cap="Some name variants of the City of Aachen according to CERL Thesaurus."}
aachen <- c("Aach","Achen", "Aix la Chapelle", "Aix-la-Chapelle")
aachen_harmonised <- c("Aachen","Aachen", "Aachen", "Aachen")
aachen_harmonisation_table <- cbind.data.frame(aachen,aachen_harmonised)
colnames(aachen_harmonisation_table) <- c("name_variants","harmonised_name")
print("A simple harmonisation table")
print(aachen_harmonisation_table)
```



## Regular Expressions and the Stringr Tools

### Cleaning Data at Scale

Cleaning data often involves making the same kind of operation over many things. For example, a very common source of spelling variation 
is that at the very end of a string (e.g "City of London.") there is a white space ("City of London ."), and this same error occurs for
tens or even hundreds of entities in our data ("Aachen .", "Paris ." etc.). A slightly more abstract problem of similar kind would be data that is consistent, but at the wrong level of granularity. If we wanted to analyse families instead of persons, but in our data people were named the following way:

```{r echo=FALSE}
example <- c("Thomas Hobbes","Maria Hobbes","John Locke","Adam Locke")
print(example)


```
Then we would need to extract the latter part of each name not to count individuals (in this case duplicate variants of the family), and
to do this over all names.

Fortunately, there is a collection of R tools that allow us to detect textual patterns of both sorts and more than we demonstrated above, 
and, after finding the patterns, to apply different kinds of operations over them.

### Using Stringr and Regular Experssions

Stringr is a R package [@Wickham2022] that is part of the Tidyverse, the collection of R tools that were introduced last week. It is not the only way to do text pattern detection and manipulation with R, but it being part of a bigger system (that we are already using) and covering the basic tools makes mastering it a good starting point for learning data harmonisation by coding.

There are seven main functions in the Str package, and all of them share a component called pattern. The pattern is written as a regular expression. Regular expression is a description of a string of text. It can be as concrete as description of the letter a (the corresponding pattern being "a"), number between 1 and 5 ([1-5]) or a letters-only piece of text at the end of the string ( [a-z]+$) These descriptions follow a certain grammar, that we will discuss more soon. Regular expressions can be used to find spelling variations, data in the wrong format and other things that make data unharmonised for your purposes. 

The functions of Stringr allow different kinds of operations over the patterns? We consider four of the functions in the String package (for those interested, all of the main 7  are shortly introduced [here](https://stringr.tidyverse.org/)) that are especially useful for harmonisation and getting to know your data in general. In all cases, the "pattern" parameter is the regular expression, and string is a character vector.

#### str_detect
Returns TRUE or FALSE based on whether it found the pattern from the text. Very useful for exploring your data.
```{r}
example <- c("Thomas Hobbes",c("Maria Hobbes"),c("John Locke"),c("Adam Locke"))
is_hobbes <- str_detect(example,pattern = "Hobbes")
print(is_hobbes)

```

#### str_subset
Returns those instances where there is match with the pattern. Useful for exploring your data.
```{r}
example <- c("Thomas Hobbes",c("Maria Hobbes"),c("John Locke"),c("Adam Locke"))
is_hobbes <- str_subset(example,pattern = "Hobbes")
print(is_hobbes)

```

#### str_replace
Replace the pattern with something else. Note that we used a slightly more complicated regular expression, that
replaces the letters only part of the string at the end, and that notices that the first letter of the family name is a capital
letter. The text that replaces the matched pattern goes to the replacement parameter.
```{r}
example <- c("Thomas Hobbes",c("Maria Hobbes"),c("John Locke"),c("Adam Locke"))
family_name_replaced <- example %>%  str_replace(.,pattern = " [A-Z]{1}[a-z]+$",replacement = " insert_family_name_here")
print(family_name_replaced)

```

#### str_extract
Returns the first match of the pattern. For example, we could use the function to turn the names in our data set to family names. Notice also that we use the str_replace function to remove the whitespace after exracting the family name.
```{r}
example <- c("Thomas Hobbes",c("Maria Hobbes"),c("John Locke"),c("Adam Locke"))
family_name_extracted <- example %>%  str_extract(.,pattern = " [A-Z]{1}[a-z]+$") %>% str_replace(.,pattern = " ",replacement = "")
print(family_name_extracted)

```

### More About Regular Expressions and Stringr in Practice 

For effective use of String and similar tools, certain familiarity with the language of regular expressions is needed. We don't aim
to teach you all about regular expressions, but enough to get you started. Don't worry about memorising everything by heart, there is an official [cheat cheet](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf). It is more important to get a touch to how
regular expressions work. The cheat sheet has both the Stringr and regular expressions basics condensed to it, so it is a good idea to bookmark or download to your computer. There are also more comprehensive Stringr tutorials like [this one](https://r4ds.had.co.nz/strings.html).

Next, we go through some examples that make use of different elements of the "grammar" of regular expressions. It might be a good idea to also look at the str_function examples again after these examples. We use str_replace, because it makes it easy to demonstrate what was the detected pattern (that is then replaced).

##### Trivial patterns

```{r}
# Matching a singe element, the letter T
example <- c("Thomas Hobbes",c("Maria Hobbes"),c("John Locke"),c("Adam Locke"))
example_replaced <- str_replace(example,pattern = "T",replacement = " match was here ")
print(example_replaced)
```


```{r}
# Matching collection of elements, e.g a name
example <- c("Thomas Hobbes",c("Maria Hobbes"),c("John Locke"),c("Adam Locke"))
example_replaced <- str_replace(example,pattern = "Thomas",replacement = " match was here ")
print(example_replaced)
```

##### Alternates

The pattern might allow for different options, which is what the examples in this subsection are about.
```{r}
# Matching either or (one of the two names in this case)
example <- c("Thomas Hobbes",c("Maria Hobbes"),c("John Locke"),c("Adam Locke"))
example_replaced <- str_replace(example,pattern = "Thomas|Maria",replacement = " match was here ")
print(example_replaced)
```

```{r}
# Matching from a range, here they are numbers from 0 to 9, but they could also be e.g. letters from a to z [a-z].
# In general, the brackets [] allow for alternative matches.
example <- c("Thomas Hobbes 1","Maria Hobbes 7","John Locke","Adam Locke")
example_replaced <- str_replace(example,pattern = "[1-9]",replacement = " match was here ")
print(example_replaced)
```

```{r}
# Matching from a collection of values, notice that we once again use brackets [] for alternatives, but this time
#without range.
example <- c("T","M","L")
example_replaced <- str_replace(example,pattern = "[TM]",replacement = " match was here ")
print(example_replaced)
```


```{r}
# The alternatives can also be used to replace anything but what is specified within the []. This happens with ^
example <- c("A","7")
example_replaced <- str_replace(example,pattern = "[^1-9]",replacement = " match was here ")
print(example_replaced)
```
#### Anchoring
The pattern can be specified to relate to certain parts of the string, namely the start or the end.
```{r}
# The match can be anchored to the start of the string with ^
example <- c("this is not a number 9","7 is a number")
example_replaced <- str_replace(example,pattern = "^[1-9]",replacement = " match was here ")
print(example_replaced)
```


```{r}
# The match can be anchored to the end of the string with $
example <- c("this is not a number 9","7 is a number")
example_replaced <- str_replace(example,pattern = "[1-9]$",replacement = " match was here ")
print(example_replaced)
```

#### Grouping
Elements in the pattern can be grouped with parentheses.
```{r}
#Search for either Maria or Thomas, and Hobbes after that
example <- c("Thomas Hobbes","Maria Hobbes","John Locke","Adam Locke")
example_replaced <- str_replace(example,pattern = "(Thomas|Maria) Hobbes",replacement = " match was here ")
print(example_replaced)
```

#### Quantification
The pattern can include a specification about the number of times something occurs in the string, this
subsection provides examples of that.
```{r}
#Replace Thomas when it occurs from two to three times.
example <- c("Thomas Hobbes","Thomas Thomas Hobbes","Thomas Thomas Thomas Hobbes")
example_replaced <- str_replace(example,pattern = "(Thomas ){2,3}",replacement = " match was here ")
print(example_replaced)
```

```{r}
#? * and + can also be used to control how many times something need to occur in a match. Here we demonstrate the use of
#+, that means that the pattern needs to be found at least once, but possibly more times.
example <- c("Thomas Hobbes","Thomas Thomas Hobbes","Thomas Thomas Thomas Hobbes")
example_replaced <- str_replace(example,pattern = "(Thomas )+",replacement = " match was here ")
print(example_replaced)
```

#### Look arounds
```{r}
# Match can be conditioned to the surrounding elements. Here, the regular expression looks for Maria followed by Hobbes.
#More ways to do this on the cheat sheet.
example <-  c("Thomas Hobbes","Maria Hobbes","John Locke","Adam Locke")
example_replaced <- str_replace(example,pattern = "Maria(?= Hobbes)",replacement = " match was here ")
print(example_replaced)
```

#### Character matching
There are some characters or character combinations with special meaning in regular expressions. We demonstrate some examples,
the cheat sheet has a more comprehensive list.
```{r}
# . matches any character except a new line. Here, we use it with + to allow any string without new lines to be a match
example <- c("Thomas Hobbes","Maria Hobbes","John Locke","Adam Locke")
example_replaced <- str_replace(example,pattern = ".+",replacement = " match was here ")
print(example_replaced)
```


```{r}
# . is an example of a character, that needs to be written as \\., if the aims is to match a dot and not reference to the special
#meaning, in this case any character, of that character in regular expressions. The same applies if one is searching e.g for [ or (.
example <- c("Thomas Hobbes.","Maria Hobbes","John Locke","Adam Locke")
example_replaced <- str_replace(example,pattern = "\\.",replacement = " match was here ")
print(example_replaced)
```


```{r}
# [:digit:] is an example of command that matches a distinct set of characters, in this case digits.
example <- c("Thomas Hobbes 12","Maria Hobbes","John Locke 7","Adam Locke")
example_replaced <- str_replace(example,pattern = "[:digit:]",replacement = " match was here ")
print(example_replaced)
```

#### Combining elements
Some of the examples have already used many of the properties of regular expressions at once, and this is often
extremely helpful, as it makes regular expressions much more expressive. The last example uses many of the things
we have dicussed at the same time.
```{r}
# Task, match the people who were born at the 17th century and have a de at their names
example <- c("Baruch de Spinoza 1632-1677","John Locke 1632-1704","François Fénelon 1651-1715")
example_replaced <- str_replace(example,pattern = ".* de .* 16[0-9]{2}-1[0-9]{3}",replacement = " match was here ")
print(example_replaced)
```
Here we first allowed string of any length (first name), then we required de, then allowed for another string of any length or type,
after which we required 16 followed by two other numbers (birth year in the 17th century), followed by - and the year of death.
And this is by no means as expressive as a regular expression can get.

### Regular Expressions and Stringr - a Summary 
Now, you should have a some sort of understanding in how regular expressions can be used in the manipulation of strings.
We will demonstrate the practical usefulness of these tools in the exercise set of this week, where we will use some of 
these tools to clean the Twitter data set you obtained last time.

## Other Solutions To Harmonisation of Data

It should also be mentioned, that data harmonisation can be done without building a code workflow. [Openrefine](https://openrefine.org/) is an example of a harmonisation tool with an user interface. We do not prohibit the use of such tools for e.g. harmonising your data set for the final project, and it can a reasonable choice in many instances. However, it will serve you in the long run to get familiar at least with regular expressions, and possibly more. Building workflows of data harmonisation often takes as much
or more time than the actual analysis in DH research, so building competence in these skills is useful.

## Missing data 

Another major theme is data that is lacking. It is very common in the humanities and the social sciences, that some of the data that should be part of the data set is not actually there. This phenomenon comes in two major forms, as
the data can be either completely or partially missing. Here, we take a quick overview to these two types of missingness, and to some of the ways the problems caused by them can be alleviated (or at least understood). 

Partially missing data is a known unknown, you can observe just by looking at the data set. In R, you might have seen value  NA in a cell of a table, where you would have expected a character or numeric value. In other words, partially missing data refers to instances where we are missing some of the values of variables for an observation. The reasons for missing data vary. People might not answer all the questions in a survey, the piece of data might be unavailable (e.g. books imprint might lack
the name of the publisher), query used to get the data might miss values in the wrong format, or any other reason. The end result is still the same: a collection of missing elements in your data.

```{r echo=FALSE}

example_data_set <- matrix(nrow=2,ncol=2) %>% as.data.frame(.)
colnames(example_data_set) <- c("name","occupation")
example_data_set[1,1] <- "Thomas Hobbes"
example_data_set[2,1] <- "Steven Steel"
example_data_set[1,2] <- "philosopher"
print(example_data_set)

```


There are two reasons why the partially missing data matters. One is purely technical: many operations can not be performed over missing values. For example, lets assume that you want to take the average number of pages for three books, but for one of the books, the number of pages is missing. As "missing" or NA is not a number, you can't sum it with numbers, hence you can not take
the average. Most of data analysis in the humanities and the social sciences would be impossible if problems like this stopped as from taking averages or from doing other sorts of analyses that can't tolerate missing values. One common solution is simply to ignore the missing values as if they did not exist in the data. In the example, this would mean that the average is taken over the number of pages of the two books with page number information available.

The other reason for why partially missing data is problematic, is that missing values might not be missing at random. For example, think of a survey that asks peoples opinions about a polarising topic: people who are not answering might do this because they think their opinion is controversial, and they don't trust the anonymity of the survey. This is one of the reasons why the simplest solution - just ignoring the missing values, more formally omission - is sometimes dangerous. Other solutions include imputation (filling of missing information with an estimate) and analyses that are designed to be less affected by missing data.

We do not teach imputation or analyses robust to missing data on this course, but we emphasize that they are motivated by a question anyone analysing their data should consider: are the missing values Missing Completely at Random (MCAR), or does data missing correlate with the properties of the data? If yes, is it possible that this affects the results of a (network) analysis done with the data? There is no out-of-the-box solutions to these problems, which makes (among other reasons) the expertise that scholars from the humanities have about their subject very valuable to data analysis as well, as they often know about such potential pitfalls in the sources they are working with (even if they don't articulate these things in terms of statistics).

Completely missing data makes the unknown unknowns related to your data, those instances that are not there at all. Like partially missing data, completely missing data can have various causes. For example, some information in social media can be hidden, meaning that queries or scraping will miss it. Historians, archeologist and others working with pre-modern societies face missing data all the time,
as many books, letters and other artifacts known to have existed have been completely lost.

Completely missing data does not create the same kind of technical problems as partially missing data, as they are not present in your database in any form. However, their implications for the meaningfulness and interpretation of, lets say, your network, can be as or even more severe than those cases that are partially missing. Entire observations can also be missing at random, or it might be related to the properties of the observations themselves. For example, there is strong evidence that the ESTC is heavily biased towards certain kinds of items before 1641, covering them extensively (e.g. bibles) while missing most examples of other types of items [@Hill2016]. In some instances, it is possible to evaluate how much and what is missing, like with the ESTC, that can be compared to other sources. However, this is not always the case.

Fortunately, it is always possible to think what your (network) data needs to be to serve your research question. For example, is it necessary that your network covers all the persons and connections that could be involved, or is a good proportion or a sample (representative set of data from the population of your interest) enough? Do the relevant results change radically if you drop some of the network edges away at random to test how sensitive the results are to some variation of the data set? If so, is it possible that your results are very sensitive to missing data as well. Very often, the research process and closer analysis of the data reveals that the original question required too much from the data, or that it is much more suited to analyse something else that was revealed during the research process. But this is a normal part of research, and should not be feared.

## For the Next Week
The treatment of missing data can lead to dramatic changes in results. The [article to read for the next week](https://www.nature.com/articles/s41586-021-03655-4) is a critique of another article, published in a very prestigious journal.
You should be able to access the article through Helka. The article is not the easiest, but try to understand the main ideas. We will discuss the article next week. Especially, everyone should be prepared to answer these four questions in front of the class. Who answers will be selected randomly during the lecture (one person per question, no more than one question per student). The questions are:

1) Q: What is the forward bias in the Seshat data according to the authors?
2) Q: How the missing information about moralising gods was handled in the R script of the original article? 
3) Q: How the predicted emergence of moralising gods differ between the revised regressions and the original Seshat data set? 
4) Q: What has happened to the original article? 

## References
   
  
