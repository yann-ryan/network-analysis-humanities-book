[["index.html", "Applying Network Analysis to Humanities About this Book About the course Reading Final Project Slides", " Applying Network Analysis to Humanities Yann Ryan, Iiro Tihonen 2022-10-21 About this Book This book is intended to be read alongside the the “Applying Network Analysis to Humanities’ course at the University of Helsinki, beginning November 2022. This course is aimed at complete beginners to both R and network analysis, although you’ll still get plenty out of it if you have experience with either. It focuses on applied approaches to network analysis and humanities data. Rather than cover network science in exhaustive detail, you’ll learn how to find, extract, clean, visualise, and analyse humanities and cultural datasets from a network perspective. Additionally, we’ll focus on the problems and pitfalls of using networks, specific to humanities data. There are two chapters for each week, one for each session. Browse through the chapters using the menu to the left. For most classes, there is an equivalent set of exercises. We’ll begin them during the class but they can be completed afterwards. You’ll probably want to have this book at hand to complete them. They are available as R markdown notebooks, using the CSC notebooks service. You’ll need a CSC account, and you’ll be sent a join code at the beginning of the course. If you don’t have either of these things, contact the course leader. About the course The course is held online, via Zoom, on Wednesdays and Fridays, between 08.15 and 09.45, starting on November 02. Assessment is the following: Completion of weekly assignments (pass/fail, carried out during class and finished afterwards if necessary). Presentation on final project (1/5 of the grade), given on Wednesday or Friday of the final week. Final project (4/5 of the grade, submitted on January 17 at the latest. Full details on the course page on Studies Service Reading Each week will have one piece of set reading, usually an article or book chapter, to be discussed in-class the following week. Week 1: Ruth Ahnert, Sebastian E Ahnert, Metadata, Surveillance and the Tudor State, History Workshop Journal, Volume 87, Spring 2019, Pages 27–51, https://doi.org/10.1093/hwj/dby033 (http://www.scottbot.net/HIAL/index.html@p=6279.html will also be helpful) Week 2: Beheim, B., Atkinson, Q.D., Bulbulia, J. et al. Treatment of missing data determined conclusions regarding moralizing gods. Nature 595, E29–E34 (2021). https://doi.org/10.1038/s41586-021-03655-4 Week 3: Week 4: Ahnert et. al. (2021). The Network Turn: Changing Perspectives in the Humanities (Elements in Publishing and Book Culture). Chapter 5, ‘Quantifying Culture’ (https://www.cambridge.org/core/elements/network-turn/CC38F2EA9F51A6D1AFCB7E005218BBE5) Week 5: Venturini, T., Jacomy, M., &amp; Jensen, P. (2021). What do we see when we look at networks: Visual network analysis, relational ambiguity, and force-directed layouts. Big Data &amp; Society, 8(1). https://doi.org/10.1177/20539517211018488 Week 6: Silvia Donker, ‘Networking Data. A Network Analysis of Spotify’s Socio-Technical Related Artist Network: Vienna Music Business Research Days’, International Journal of Music Business Research 8, no. 1 (1 April 2019): 67–101. https://pure.rug.nl/ws/portalfiles/portal/96957258/volume_8_no_1_april_2019_donker_end.pdf (http://www.scottbot.net/HIAL/index.html@p=41158.html will also be helpful) Week 7: Mushon Zer-Aviv, ‘If Everything Is a Network, Nothing Is a Network’, Visualising Information for Advocacy, accessed 17 October 2022, https://visualisingadvocacy.org/blog/if-everything-network-nothing-network. (https://raley.english.ucsb.edu/wp-content/Engl800/Galloway-some-things-unrepresentable.pdf also interesting) There is no set textbook, but the following are good general introductions to networks: Ahnert, R., Ahnert, S., Coleman, C., &amp; Weingart, S. (2021). The Network Turn: Changing Perspectives in the Humanities (Elements in Publishing and Book Culture). Cambridge: Cambridge University Press. doi:10.1017/9781108866804 Short, very readable volume on networks, specifically focused on humanities applications. Free, open access copy available through the above link. Easley, D., and Kleinberg,J. (2010). Networks, Crowds, and Markets: Reasoning about a Highly Connected World. Cambridge University Press. Very comprehensive textbook on networks, mostly relating to economics, sociology, computing. Pre-publication draft is available for free on the book website. Barabási, A.-L. (2002). Linked: The New Science of Networks. Perseus Pub. Popular science book on networks, very influential in bringing the science of networks to a popular audience. Newman, M. E. J. (2018). Networks (Second edition). Oxford University Press. Comprehensive textbook of network theory, recommended if you want to understand algorithms etc. in more detail. Also worth checking out is the extensive bibliography and journal by the Historical Network Research Community. Final Project The final project is due on December 23. You can start on the project as early as you like. To help you with your project, you’ll present your ideas and proposed methods in the final week of the course. At this point, you should hopefully have some preliminary research and outputs, or at the least a plan for your project and how it will come together, which you will communicate to the group in a presentation. Because of this, it’s recommended to begin preliminary work on your project a couple of weeks before this. You’ll also be asked to give feedback and ask questions of the other class participants, with the aim of helping their own projects. The final project tasks you with using your network data and related data model to carry out an analysis. The project should take the form of an R Markdown notebook, which you’ll learn how to create over the next few weeks. An R Markdown document is a format which allows you to combine text, chunks of code, and the output of those chunks. You’ll write up this document and then turn it into a HTML page - a process known as ‘knitting’. Slides There are also a set of slides for each week, which you can access here: Week 1: class 1, class 2 Week 2: class 1, class 2 Week 3: class 1, class 2 Week 4: class 1, class 2 Week 5: class 1, class 2 Week 6: class 1, class 2 Week 7: class 1 "],["week-1-class-1-course-introduction.html", "Week 1, class 1: Course Introduction Introduction How to use this book and follow the course. What do we mean by a ‘network’? Why Use Networks? Networks in the Humanities Graph Theory Why use networks in humanities? Network Basics Network paths Different Network Types From Bridges to Social Networks… The ‘New’ Science of Networks Conclusion", " Week 1, class 1: Course Introduction Slides for this week Introduction This course will teach you how to use networks to ask (and hopefully answer) questions relating to humanities. More specifically than this, you’ll use the science of networks to model entities, and the relationships between them. To do this, you’ll use your knowledge of a subject to build a data model: a way to conceptualise the way your data all fits together, in a way that allows you to extract network data from it. How to use this book and follow the course. Each week, you will need to complete a short assignment. This takes the form of an editable R notebook, which can be found on your CSC notebooks workspace. Soon, you’ll log into the CSC notebooks server, and open a source copy of this section, which is interactive, allowing you to edit and run code. After each week, you’ll open the interactive notebook, complete the assignment, and upload the file. As we haven’t introduced R yet, for this week, your only task is to make a copy of the exercise in your personal folder, ‘knit’ it, and send the resulting file to the course leader. In the following weeks, this is the method you’ll need to use to submit the assignments, so this is an opportunity to familiarise yourself with it, and to iron out any problems. What do we mean by a ‘network’? The word ‘network’ is ubiquitous in our daily lives. The Oxford English Dictionary tells us the term itself originally was first defined as Work (esp. manufactured work) in which threads, wires, etc., are crossed or interlaced in the fashion of a net. (think patch work), but now we commonly use it for any complex system of interrelated things: we all use social networking, and the telephone, railway, and road networks; an area of a city might be described as having networks of alleyways, or a historian might write about a network of trading posts. These are all in some way metaphorical. The networks we’ll learn about and use on this course have a more specific definition. In mathematics, a network is an object made up of things and connections. To use the standard language, the things are called nodes (or sometimes vertices), and the connections called edges. These things (and connections) could be almost anything. Some typical examples include: Nodes are people, and the edges their friendships or followers, as in a typical social network. The nodes are books and authors, and the connection is ‘written by’. Nodes are web pages on the internet In this final example, what might the connections be? The connections between web pages are generally the hyperlinks: the system of links known as the World Wide Web. Why Use Networks? This is a very valid question, and there are plenty of circumstances when a network model is not the best was of representing a dataset, as we’ll learn. However, when networks are used correctly, they have a number of benefits: Networks allow us to reduce or understand complexity: we can reduce complicated data to its overall structure. Network visualisations, when used well, allow us to spot patterns and make inferences on the structure of our data. Rather than consider each individual data point as isolated, networks allow us to consider how data is interrelated, and how the relationships between nodes effect each other and the overall structure. Network techniques allow us to move beyond pairs to putting entities and their relationships in a much larger context, showing how they bridge the local and the global. They allow us to consider groups within data, particularly in communication networks. Networks can point to data subsets worth investigating in more detail: allowing us to move from close to distant reading, and in-between. Networks in the Humanities Networks have long been used to understand aspects of historical or cultural change. In below video, accompanying a paper in Science, researchers graphed places of birth and death—taken from Wikidata—as nodes and edges, using it as a way to trace the movement of culture from one area of the world to another, and how this changed over time. Given that the information is taken from Wikidata, what problems could you envisage with this study? While Wikidata is a large data source, it is obviously very partial and has a particular focus, for example in favour of white, male, Western Europeans. The study may just replicate the biases of Wikidata itself, hiding the important cultural influences on the west, from other parts of the world. Other typical uses of complex networks in humanities subjects include spatial networks, line of sight or pottery similarity networks in archaeology, historic road networks, and character networks in plays. In this course, we’ll mostly work with networks as tools for descriptive and exploratory data analysis. But network science also has wide uses in statistical and predictive modelling. Some Examples Before we get in to some of the details, it’s worth going through a few examples of interesting projects which have combined humanities data with network science. Tudor networks Ahnert and Ahnert used a number of network measurements to make a ‘network fingerprint’ for individuals in an archive of Tudor state papers. Using this measure fingerprint, they were able to identify or confirm spies and collaborators. Specifically, they looked at individuals with a high betweenness (a bridging measurement) in comparison to their degree (measuring their overall connections): the points underneath the trendline in the graph below. The biggest outlier is Jon Snowden: an alias of John Cecil, a Catholic priest in exile in Elizabeth I’s regime. Looking at others with a similar profile revealed other conspirators. Using Multi-Layered Networks to Disclose Books in the Republic of Letters This project made a ‘multi-layer network’ by combining a network of letters with a network of book mentions. Six Degrees of Francis Bacon The Six Degrees of Francis Bacon project used co-occurrence networks (a technique we’ll learn about later) to infer links between early modern individuals. Specifically, they looked for co-occurrence of individuals in the Oxford Dictionary of National Biography, inferring links if two individuals were mentioned together more than was statistically significant. This data was then turned into an interactive social network. Mapping the digital humanities community of Twitter Measured the centrality of a group of 2,500 Twitter users self-identified as digital humanities practioners Looked a separate communities of languages, French was particularly isolated. Graph Theory Representing data in this way allows us to use the mathematics of graph theory to find out things about it. The origins of this date back to the 18th century, and the mathematician Leonard Euler. The city of Königsberg (now Kaliningrad) was built on a river with two islands and a system (a network, perhaps?) of seven bridges connecting them. The inhabitants of the city had long wondered if there it was possible to devise a route which would cross each bridge exactly once. Euler solved this problem by abstracting the bridge system into what we now call a graph: a structure with nodes and edges. He used this approach to prove that the path was only possible if the graph had exactly zero or two nodes with a degree (a count of the total connections) with an odd number. This process: abstracting a system to its connections, and devising mathematical rules to understand it, forms the basis of modern graph theory. Graph theory has methods to perform all sorts of calculations on the network: both on the individual nodes (for example to find the most important nodes by various measurements), and on the network as a whole. Rather than think about each individual relationship separately, a network model means that we can understand some more about how each part is interrelated, how the structure itself is important, and how one node might affect another in complex ways. Why use networks in humanities? Network analysis has become one of the most popular (and perhaps one of the most overused) techniques in the digital humanities toolbox. Over this course, we’ll ask you to think critically about the methods you use and when they might not always be the best way to think about your data. Almost anything can be a network, but should you use network analysis? Here is a handy flowchart which might help you decide. At the same time, network analysis does have plenty to offer the digital humanist: Humanities data is messy and complex, and network analysis deals well with complexity. The ubiquitous ‘hairball graph’, for example, might in some cases be the best way to get an overview of the structure of a large set of humanities data. Human relationships are naturally interrelated. The reason two individuals might exchange correspondence is not generally random, but strongly dependent on a wider network, for example whether they have friends in common. Networks can help to untangle these dependencies. Network analysis doesn’t always provide the answers, but it can be a way to filter down to particularly important individuals or relationships in a dataset, worthy of a further look with your humanities hat on. There are lots of other ways the tools of networks are useful, where understanding the network itself is not the end goal. For example, knowledge graphs, or certain techniques for information retrieval. Network Basics This section introduces the fundamental basics of network analysis, including its key components, and various network types. Nodes and Edges As already mentioned, a network is a graph consisting of nodes and edges (connections). Typically (though not always), the edges are pairwise, meaning they run between a pair of nodes. These are often represented visually as points (the nodes) and lines (the edges), like this: Edge weights These connections can often have a weight attached, for example the number of letters exchanged between two people, or the number of times two actors appear in a scene in a movie or play together. A weight might also be a measurement of similarity or difference between two nodes, such as a linguistic similarity between two books (a network can also be between inanimate objects!), or the distance between two cities on a map. These weights can be used in the calculations. Edge direction Edges can also have directions, meaning that the incoming and outgoing links are counted separately: for example we might count incoming and outgoing letters separately. What might this letter count tell us about a relationship in a network? What potential danger is there is reading something from this? It might be worth considering how the letter network was constructed. Often they are reconstructed from personal letter archives, which tend to be collections of mostly incoming letters to a single person or family. In that case, it’s likely the difference between incoming and outgoing letters is not statistically significant, but simpy a product of the method of data collection. Weighted and directed These directed edges can also have separate weights attached to them: Network paths One of the central concepts behind networks is that they allow information to travel along the edges, moving from node to node. In a metaphorical sense, nodes with less ‘hops’ between them have an easier route to this information, and may be said to be close together or influential on each other. If we add more than three nodes to the network, these paths begin to emerge. A network path is simply a route, travelling along edges, from one node to another in a network. Some network metrics use these paths to estimate structural importance, for example. Paths work differently in directed networks: information can only move in the direction of the edges. Figure 1: Network diagram showing the shortest path between node 5 and node 6. Left-hand network is undirected, meaning the path can travel along any edge. Network on the right is directed, meaning a path only exists in the direction of the edge. Different Network Types Bipartite networks In the above examples, the nodes and connections have been very straightforward: two things of the same type (people in a social network for example) connected to each other. These are known as one-mode or unipartite networks. However, many networks you’ll encounter will be of things of different types. These are known as bipartite networks, and we’ll return to them later in the course. They are very common in humanities data. Some examples include: A network of characters in a play connected to scenes they appear in. A network of company directors connected to companies A network of publishers connected to the books they financed. A network of people connected to membership of certain organisations The diagram below is an illustration of how this looks, with a network of publishers connected to books. Each individual is listed as the publisher of a number of books, which become the two node types, connected as shown in the figure on the left. A publisher can be connected to many books, but the two types of nodes cannot be connected to each other (a publisher obviously can’t publisher another publisher…). Many network measurements and algorithms are designed to be used on regular, one-mode networks. Often, when working with bipartite networks, we collapse the network into one of the node types, meaning that we directly connect one of the types to each other, based on shared connections to the other type. In the figure on the left below, we have collapsed the network so that now, publishers are directly connected to each other, based on shared appearances on books. If publishers shared multiple books, this can be added to the new edge as weight. What would a projected network of the other type (books connected to books) look like? All three books would be connected to each other, because they all have one publisher in common (Strahan). Book 1 and book 3 would have an edge weight of 2, because they have two publishers in common (Millar and Strahan). Multigraphs There can also be multiple kinds of edges, in the same network. For example in a network of historical correspondents, you might have ‘met in person’ as well as ‘sent a letter to/from’. If these edges are not merged, these graphs are known as multigraphs. Figure 2: A diagram of a multigraph: a network with edges of more than one type. Multigraphs are also known as multi-layer networks: you could imagine each separate set of edges as a separate layer within a network. Conceptually, we might learn interesting things by understanding how the various layers overlap and interact. R and other programming languages have packages to plot and analyse them. Hypergraphs The final type of network is even more exotic: in an ordinary network, an edge always connects two nodes together. A network where each edge can connect to any number of nodes is called a hypergraph. It’s best explained with a diagram: in the figure below, an edge is no longer a line but a coloured area, and each of the points which fall within them are connected nodes. Good real-world examples are WhatsApp or Facebook groups: each person can be a member of multiple groups. (By Hypergraph.svg: Kilom691derivative work: Pgdx (talk) - Hypergraph.svg, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=10687664) What other way could we model the network of Facebook or WhatsApp groups? These could also be considered bipartite networks (individuals connected to groups). These networks—multigraphs and hypergraphs—require an additional set of algorithms, and generally off-the-shelf tools have not been developed for them in the same way as exist for normal graphs. However, there are packages available for R which have been developed to deal with them. Spatial networks A final type of network is a spatial network: one where the nodes and edges have real-world spatial characteristics. A good example of this is a road network: edges can be represented as geometric lines, and cities and other points of interest as geographic points. These can have historical uses, for example the early modern road network of some of north-western Europe has been mapped by the Viabundus project as a spatial network: using simple network shortest-path algorithms and adding the road length as a weight, the data can be used to plot likely itineraries from one point to another. A map of the early modern road network made with data from the Viabundus project. Each color represents the ‘neighbourhood’ of roads which can be reached from Urecht in a single day. From Bridges to Social Networks… In the twentieth century, this graph theory began to be used by the new discipline of sociology now applied to human relationships, to understand the processes behind business, family, and friendship ties. This gave birth to the field of ‘social network analysis’, which over the past half a century or so has developed a whole range of theories governing the ways networks of people are formed, and what implications this has for the way they act. Human nodes naturally behave quite differently to an island, after all. In what specific way might the connection between two people have different attributes to a connection between two islands? Bridges and islands, clearly, don’t have any choices as to whom they will connect to: it is determined by their geographic position. So many network analysis techniques may not be appropriate. But it is a good example of how graph theory should be considered firstly as a set of mathematical tools. The Strength of Weak Ties We won’t deal with all of these theories here, but will mention a few key ones which are good demonstrations of the way in which graph theory has been applied to social networks, and the kinds of things it has determined.. One of the pioneers of social network analysis was Mark Granovetter. In a 1973 paper, Mark Granovetter argued that the most important ties in a network are often those which connect two separate social groups together. These ties, according to Granovatter, occupy a ‘brokerage’ position, and can be key in certain situations. For example, paradoxically, job seekers are more likely to find the most useful leads through their distant acquaintances rather than their close friends… Why might this be the case? It’s because, according to Granovetter, these distant acquaintances are more likely to be able to provide ‘new’ information on opportunities: a close friend, on the other hand, will probably have access to the same information as you. This ‘brokerage’ position can be deduced mathematically using a metric known as betweenness centrality (we’ll come back to that in a later class). It’s a Small World Another important early finding of social network analysis came from a series of experiments by the social psychologist Stanley Milgram. In 1967 Milgram devised an experiment where a series of postcards were mailed out to random people in US cities. These postcards contained basic information about a ‘target’ person in another, geographically-distant, city. The participants were asked to send the postcard to that person if they knew them, and if not, send it to the contact they thought might be most likely to know that person. The details of each person were recorded on the postcard at every step. What problems can you imagine with this experiment? It’s worth considering how the experiment may be biased. Are all groups of people equally likely to answer (or have the time and money to carry out) a request from a random postcard? How might this have distorted the findings? When (or if) a postcard made it to the target person, Milgram could see how many ‘hops’ it had taken in order to get there. The average number of hops was between five and a half and six: this information was later used to claim that everyone in the US was connected by ‘six degrees of separation’. In network terms, this is known as the ‘average path length’ of the network. This fact is known as the ‘small world’ effect. It is also connected to the ‘strength of weak ties’ theory by Granovetter. This surprisingly-small number is possible because of the structure of social networks: if you want to reach someone in a distant city, are you more likely to have success if you send it to a close neighbour, or a distant acquaintance who lives there? The ‘New’ Science of Networks The most common use of networks in academic research much of the 20th century was looking at these small, sociological networks of relationships between people. This changed in the late 1990s, when a group of scientists began to use network research to understand the structures governing many kinds of complex systems, initially using the approach to map out the structure of the World Wide Web. This research showed that many of these complex networks had a similar structure: a small number of nodes with a very large number of connections, known as hubs, and a large number of nodes with very few connections. In fact, they argued, the structure followed what is known as a power-law: essentially many nodes have a very small number of connections, an exponentially smaller number of nodes have an exponentially larger number of connections, and so forth until a tiny of number of nodes have very many connections. These researchers, notably Albert Barabasi, argued that this process was guided by preferential attachment, meaning that these networks were created because nodes in a network were much more likely to attach themselves to nodes which already had many connections, leading to a ‘rich get richer’ effect. Perhaps most importantly, they demonstrated empirically that this particular structure could be found across a range of networks, from computers, to people, to biological networks or the structure of neurons in the brain. And they have wide-ranging implications: a scale-free network means it is easy for a disease to spread, because once it reaches a hub node it can easily move throughout the network. Figure 3: These diagrams show how a scale-free network is susceptible to to targeted attack but not to random. Removing any two nodes at random in a network of hubs makes little difference to the overall structure or robustness. However, removing two hub or bridging nodes, as on the right, results in a network in which many of the paths are now severed, meaning the network structure has begun to fall apart. Images from https://cs.brynmawr.edu/Courses/cs380/spring2013/section02/slides/01Introduction.pdf These ideas, published at a time when the internet (and later social networking) moved into the mainstream, drove a huge interest in thinking about the world in a ‘networked’ way. They spawned a range of popular science and psychology books. Barabasi wrote Linked, one of the defining popular science books of the 2000s, and used networks to argue that this scale-free network structure could be used to explain a whole range of human behaviours, from epidemics, to economics, to politics. Conclusion This outline has hopefully got you thinking about this network approach to data. A word of warning: almost everything can be represented as some kind of network. However, many of the findings from them require some careful thought: do they tell us something interesting, such as why a group of contacts formed in a particular formed in this way, or do they just reflect the data that we have collected or have available? What does a complex ‘hairball’ network diagram really tell us? Have the claims of Linked really helped us to understand business, economics, and the spread of disease? Between now and the next class, try to consider the ways some of the data you have used in your studies might be thought of as a network, and what benefits (and pitfalls) that approach might bring. Lastly, remember that a network is a model: a set of proposals to explain something about the real world. It is an artificial contruction, or a metaphor, rather than the thing itself. As such, even a very ‘accurate’ network is likely to be only one of many such models which could be used as an explanation for a particular phenomenon or observation. Throughout this course, you should use your critical tools to keep this in mind, and assess the usefulness of a particular model to your data explorations. "],["week-1-class-2-introduction-to-r-and-the-tidyverse.html", "Week 1, Class 2: Introduction to R and the Tidyverse Exercises R and R-Studio Using R Tidyverse Reading in external data", " Week 1, Class 2: Introduction to R and the Tidyverse Exercises This lesson has a corresponding editable notebook containing some exercises. These are stored as a separate notebook in the CSC notebook workspace, with the name 1-2-intro-to-r.Rmd. You’ll need to move a copy to your ‘my-work’ folder in your CSC notebook Workspace, complete the exercises, and knit them, using the method we learned in the previous class. Once this is done, send a copy of the HTML file to the course leaders. R and R-Studio Throughout this course, we’ll mostly work on networks and data using the programming lanugage R and a popular extension known as ‘the tidyverse’. This will be done using R-Studio, an interface designed to make R easier to work with, known as an IDE. For this course, the data, files, and interface are all already set up for you in the CSC Notebooks workspace. In most cases, you will want to install R and R-Studio on your local machine. See here for instructions on how to do this. Logging into CSC notebooks and opening a notebook. The first thing you should do is log in to CSC Notebooks, and start the RStudio application, [as explained in the previous chapter]. Once you’ve done this, and opened the relevant notebook, you’ll see this screen (I’ve overlaid squares and numbers to refer to different parts). R-Studio is divided into four different sections, or panes. Each of these also has multiple tabs. Starting from the top-left (numbered 1): The source editor. Here is where you can edit R files such as RMarkdown or scripts. The environment pane will display any objects you create or import here, along with basic information on their type and size. This pane has a number of tabs. The default is files, which will show all the files in the current folder. You can use this to import or export additional files to R-Studio from your local machine. The console allows you to type and execute R commands directly: do this by typing here and pressing return. All four of these panes are important and worth it’s worth exploring more of the buttons and menu items. Throughout this course, you’ll complete exercises by using the source editor to edit notebooks. As you execute code in these notebooks, you’ll see objects pop into the environment pane. The console can be useful to test code that you don’t want to keep in a document. Lastly, getting to know how to use and navigate the directory structure using the files pane is essential. Using R ‘Base’ R. Commands using R without needing any additional packages are often called ‘base’ R. Here are some important ones to know: You can assign a value to an object using = or -&gt;: x = 1 y &lt;- 4 You can do basic calculations with +, -, * and /: x = 1+1 y = 4 - 2 z = x * y z ## [1] 4 You can compare numbers or variables using == (equals), &gt; (greater than), &lt;, (less than) != (not equal to). These return either TRUE or FALSE: 1 == 1 ## [1] TRUE x &gt; y ## [1] FALSE x != z ## [1] TRUE Basic R data structures It is worth understanding the main types of data that you’ll come across, in your environment window. First, you’ll have dataframes. These are the spreadsheet-like objects which you’ll use in most analyses. They have rows and columns. Next are variables. A variable is assigned to a name, and then used for various purposes. You’ll often hear of an item called a vector. A vector is a list of objects of the same type. A vector can be a single column in a dataframe (spreadsheet), which means they are used very often in R to manipulate data. A vector can have different types: for example, a character vector looks like this c(\"apples\", \"bananas\", \"oranges\"). A vector is created with the command c(), with each item in the vector placed between the brackets, and followed by a comma. If your vector is a vector of words, the words need to be in inverted commas or quotation marks. fruit = c(&quot;apples&quot;, &quot;bananas&quot;, &quot;oranges&quot;, &quot;apples&quot;) colour = c(&quot;green&quot;, &quot;yellow&quot;, &quot;orange&quot;, &quot;red&quot;) amount = c(2,5,10,8) You can create a dataframe using the data.frame() command. You just need to pass the function each of your vectors, which will become your columns. We can also use the glimpse() or str() commands to view some basic information on the dataframe (particularly useful with longer data). fruit_data = data.frame(fruit, colour, amount, stringsAsFactors = FALSE) glimpse(fruit_data) ## Rows: 4 ## Columns: 3 ## $ fruit &lt;chr&gt; &quot;apples&quot;, &quot;bananas&quot;, &quot;oranges&quot;, &quot;apples&quot; ## $ colour &lt;chr&gt; &quot;green&quot;, &quot;yellow&quot;, &quot;orange&quot;, &quot;red&quot; ## $ amount &lt;dbl&gt; 2, 5, 10, 8 Data types Notice that to the right of the third column, the amount, has &lt;dbl&gt;under it, whereas the other two have `. That’s because R is treating the third as a number and others as a string of characters. It’s often important to know which data type your data is in: you can’t do arithmetic on characters, for example. R has 6 data types: character numeric (real or decimal) integer logical complex Raw The most commonly-used ones you’ll come across are character, numeric, and logical. logical is data which is either TRUE or FALSE. In R, all the items in a vector are coerced to the same type. So if you try to make a vector with a combination of numbers and strings, the numbers will be converted to strings, as in the example below: fruit = c(&quot;apples&quot;, 5, &quot;oranges&quot;, 3) glimpse(fruit) ## chr [1:4] &quot;apples&quot; &quot;5&quot; &quot;oranges&quot; &quot;3&quot; Tidyverse Most of the work in these notebooks is done using a set of packages developed for R called the ‘tidyverse’. These enhance and improve a large range of R functions, with much nice syntax - and they’re faster too. It’s really a bunch of individual packages for sorting, filtering and plotting data frames. They can be divided into a number of diferent categories. All these functions work in the same way. The first argument is the thing you want to operate on. This is nearly always a data frame. After come other arguments, which are often specific columns, or certain variables you want to do something with. library(tidyverse) Here are a couple of the most important ones select(), pull() select() allows you to select columns. You can use names or numbers to pick the columns, and you can use a - sign to select everything but a given column. Using the fruit data frame we created above: We can select just the fruit and colour columns: select(fruit_data, fruit, colour) ## fruit colour ## 1 apples green ## 2 bananas yellow ## 3 oranges orange ## 4 apples red Select everything but the colour column: select(fruit_data, -colour) ## fruit amount ## 1 apples 2 ## 2 bananas 5 ## 3 oranges 10 ## 4 apples 8 Select the first two columns: select(fruit_data, 1:2) ## fruit colour ## 1 apples green ## 2 bananas yellow ## 3 oranges orange ## 4 apples red group_by(), tally(), summarise() The next group of functions group things together and count them. Sounds boring but you would be amazed by how much of data science just seems to be doing those two things in various combinations. group_by() puts rows with the same value in a column of your dataframe into a group. Once they’re in a group, you can count them or summarise them by another variable. First you need to create a new dataframe with the grouped fruit. grouped_fruit = group_by(fruit_data, fruit) Next we use tally(). This counts all the instances of each fruit group. tally(grouped_fruit) ## # A tibble: 3 × 2 ## fruit n ## &lt;chr&gt; &lt;int&gt; ## 1 apples 2 ## 2 bananas 1 ## 3 oranges 1 See? Now the apples are grouped together rather than being two separate rows, and there’s a new column called n, which contains the result of the count. If we specify that we want to count by something else, we can add that in as a ‘weight’, by adding wt = as an argument in the function. tally(grouped_fruit, wt = amount) ## # A tibble: 3 × 2 ## fruit n ## &lt;chr&gt; &lt;dbl&gt; ## 1 apples 10 ## 2 bananas 5 ## 3 oranges 10 That counts the amounts of each fruit, ignoring the colour. filter() Another quite obviously useful function. This filters the dataframe based on a condition which you set within the function. The first argument is the data to be filtered. The second is a condition (or multiple condition). The function will return every row where that condition is true. Just red fruit: filter(fruit_data, colour == &#39;red&#39;) ## fruit colour amount ## 1 apples red 8 Just fruit with at least 5 pieces: filter(fruit_data, amount &gt;=5) ## fruit colour amount ## 1 bananas yellow 5 ## 2 oranges orange 10 ## 3 apples red 8 sort(), arrange() Another useful set of functions, often you want to sort things. The function arrange() does this very nicely. You specify the data frame, and the variable you would like to sort by. arrange(fruit_data, amount) ## fruit colour amount ## 1 apples green 2 ## 2 bananas yellow 5 ## 3 apples red 8 ## 4 oranges orange 10 Sorting is ascending by default, but you can specify descending using desc(): arrange(fruit_data, desc(amount)) ## fruit colour amount ## 1 oranges orange 10 ## 2 apples red 8 ## 3 bananas yellow 5 ## 4 apples green 2 If you `sortarrange() by a list of characters, you’ll get alphabetical order: arrange(fruit_data, fruit) ## fruit colour amount ## 1 apples green 2 ## 2 apples red 8 ## 3 bananas yellow 5 ## 4 oranges orange 10 You can sort by multiple things: arrange(fruit_data, fruit, desc(amount)) ## fruit colour amount ## 1 apples red 8 ## 2 apples green 2 ## 3 bananas yellow 5 ## 4 oranges orange 10 Notice that now red apples are first. left_join(), inner_join(), anti_join() Another set of commands we’ll use quite often in this course are the join() ‘family’. Joins are a very powerful but simple way of selecting certain subsets of data, and adding information from multiple tables together. Let’s make a second table of information giving the delivery day for each fruit type: fruit_type = c(&#39;apples&#39;, &#39;bananas&#39;,&#39;oranges&#39;) weekday = c(&#39;Monday&#39;, &#39;Wednesday&#39;, &#39;Friday&#39;) fruit_days = data.frame(fruit_type, weekday, stringsAsFactors = FALSE) fruit_days ## fruit_type weekday ## 1 apples Monday ## 2 bananas Wednesday ## 3 oranges Friday This can be ‘joined’ to the fruit information, to add the new data on the delivery day, without having to edit the original table (or repeat the information for apples twice). This is done using left_join. Joins need a common key, a column which allows the join to match the data tables up. It’s important that these are unique (a person’s name makes a bad key by itself, for example, because it’s likely more than one person will share the same name). Usually, we use codes as the join keys. If the columns containing the join keys have different names (as ours do), specify them using the syntax below: joined_fruit = fruit_data %&gt;% left_join(fruit_days, by = c(&quot;fruit&quot; = &quot;fruit_type&quot;)) joined_fruit ## fruit colour amount weekday ## 1 apples green 2 Monday ## 2 bananas yellow 5 Wednesday ## 3 oranges orange 10 Friday ## 4 apples red 8 Monday In this new dataframe, the correct weekday is now listed beside the relevant fruit type. Piping Another useful feature of the tidyverse is that you can ‘pipe’ commands through a bunch of functions, making it easier to follow the logical order of the code. This means that you can do one operation, and pass the result to another operation. The previous dataframe is passed as the first argument of the next function by using the pipe %&gt;% command. It works like this: fruit_data %&gt;% filter(colour != &#39;yellow&#39;) %&gt;% # remove any yellow colour fruit group_by(fruit) %&gt;% # group the fruit by type tally(amount) %&gt;% # count each group arrange(desc(n)) # arrange in descending order of the count ## # A tibble: 2 × 2 ## fruit n ## &lt;chr&gt; &lt;dbl&gt; ## 1 apples 10 ## 2 oranges 10 That code block, written in prose: “take fruit data, remove any yellow colour fruit, count the fruits by type and amount, and arrange in descending order of the total” Plotting using ggplot() The tidyverse includes a plotting library called ggplot2. To use it, first use the function ggplot() and specify the dataset you wish to graph using data =. Next, add what is known as a ‘geom’: a function which tells the package to represent the data using a particular geometric form (such as a bar, or a line). These functions begin with the standard form geom_. Within this geom, you’ll add ‘aesthetics’, which specify to the package which part of the data needs to be mapped to which particular element of the geom. The most common ones include x and y for the x and y axes, color or fill to map colors in your plot to particular data. ggplot is an advanced package with many options and extensions, which cannot be covered here. Some examples using the fruit data: Bar chart of different types of fruit (one each of bananas and oranges, two types of apple) ggplot(data = fruit_data) + geom_col(aes(x = fruit, y = amount)) Counting the total amount of fruit: ggplot(fruit_data) + geom_col(aes(x = fruit, y = amount)) Charting amounts and fruit colours: ggplot(data = fruit_data) + geom_bar(aes(x = fruit, weight = amount, fill = colour)) Reading in external data Most of the time, you’ll be working with external data sources. These most commonly come in the form of comma separated values (.csv) or tab separated values (.tsv). The tidyverse commands to read these are read_csv() and read_tsv. You can also use read_delim(), and specify the type of delimited using delim = ',' or delim = '/t. The path to the file is given as a string to the argument file=. df = read_csv(file = &#39;sample_network.csv&#39;) # Read a .csv file as a network, specify the path to the file here. ## Rows: 7 Columns: 3 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): from, to ## dbl (1): weight ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. df ## # A tibble: 7 × 3 ## from to weight ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A B 5 ## 2 A C 2 ## 3 B C 4 ## 4 A D 1 ## 5 A E 8 ## 6 E D 2 ## 7 C D 3 Notice that each column has a data type beside it, either for text or for numbers. This is important if you want to sort or run calculations on the data. This has been a very quick introduction to R. There are lots of places to learn more, including: R-studio cheat sheets The Pirate’s Guide to R, a good beginners guide to base R R for data science, which teaches the tidyverse in detail Learn how to make a book like this using Bookdown "],["week-2-class-1-acquiring-and-working-with-network-data.html", "Week 2, Class 1: Acquiring and Working With Network Data Introduction Presenting Networks as Data ESTC Tweetsets and hydrator Instructions for homework References", " Week 2, Class 1: Acquiring and Working With Network Data Introduction The topic of the second week is data. We will present the data sets available for the final project of the course, and give some practical examples of the kinds of networks that can be derived from them. We will also start discussing the data wrangling steps that are necessary to transform a given data set to a network data set. Presenting Networks as Data Before jumping into discussing data, we quickly introduce few terms terms that are helpful for understanding how data sets of various sorts can be converted to networks. Network is made of nodes and edges, so both need to be defined in machine-readable form. Although not necessary in all presentations of network data, node list is a good starting point. It is a table on N rows and C columns, where N is the number of nodes in your data. Each row, then has information about one node. One of the columns is reserved for the identifier of your nodes (a value that defines the node uniquely, think of it as the social security number of the node), the rest are optional. For example, a node list for a social network of humans would have an identifier node at one column of each row, and possible additional information on other rows (name, occupation etc). Below, we have defined the nodes - persons - in a very small network of academics. In addition to their identifier, the node list includes their occupations. id occupation person_1 humanist person_2 digital humanist person_3 computer scientist person_4 post humanist The most important element - which is in itself enough to define a network - is edge list. Edge list is a table with at least two columns. The columns can be called “from” and “to”, and the values of these columns are unique identifiers of nodes. Each row of the edge list describes one edge in the network, from the node in the “from” column to the node in the “to” column. There can be additional columns in the edge list as well, like columns describing the “strength” of the edge. In the coming weeks, we will teach you how the edge list (and optionally also a node list) can be given as a input to R functions to create a network. Now we can define the connections (let’s say, number of shared publications) between the four researchers with an edge list. It looks like this when printed in R: from to weight person_1 person_4 1 person_2 person_3 2 person_1 person_3 1 person_1 person_2 1 Adjacency matrix is a N X N table where element depicts the similarity between two things. The values in the matrix go from 0 (no similarity at all) to any positive real number. In network analysis, adjacency matrix is sometimes used to store similarities between nodes. So if you have a node list that goes from 1. to Nth row (so N nodes in total), then the cell in row i ja and column j of the adjacency matrix tells how similar ith node is to the jth node. If the network is undirected, the cell in row i and column j has the same value as the cell in row j and column i, but this is not necessarily the case with directed network. Adjacency matrix is often the easiest way to store comparisons of nodes to each other, and R makes it easy to convert it to an edge list. The adjacency matrix corresponding with the preceding edge list looks like this: person_1 person_2 person_3 person_4 person_1 0 0 1 1 person_2 0 0 2 0 person_3 1 2 0 0 person_4 1 0 0 0 Finally, the network defined by our example edge and node is presented below. Notice that one of the edges is slightly thicker than the others, can you figure out the reason for that based on the edge list? Very often, it is the case that the data you are working with is not formatted to node and edge lists from the start, and some wrangling of data is needed to get there. We provide some examples of what the process of turning data to network data can look like in the coding exercises later this week. The questions, however, remain the same. What are the nodes of your data, what are the ids of the nodes, and what are the attributes of the nodes relevant for your analysis. Make a node list of these variables. The second question is how the edges are defined in your data set. It is often first easier to make an adjacency matrix to compare the nodes to each other, and then turn the adjacency matrix to edge list, but in practice the process of creating an edge list can take many forms. The defining of edges and their strength can also require a great deal of substance understanding and thinking related to the research question. The following weeks will make these steps more concrete, and the first step towards that is to get to questions about the actual data we will be working with. ESTC The English Short Title Catalogue (ESTC) is an union catalogue of early modern British Print Products. It is the largest collection of metadata about early modern books and pamphlets of the English speaking world. Metadata in general means data about data, and in the case of books (data, in some sense) data about them. This metadata has hundreds of fields, but the big idea is to connect editions (single print runs of books) of print products to information that is relevant for contextualising them, like publication year, publication place, author publisher etc. The original intention of the ESTC was to help historians and other users of libraries with historical collections to find relevant research material, or to check basic facts about the source material they were working with. The original ESTC is managed by the British Library and the University of California Riverside, but the version we will be using during the course is a worked on by researchers of the Computational History Group at the University of Helsinki for the last ten-ish years. The ESTC has been extensively harmonised (a process relevant in all data analysis that we will discuss more during next lecture) and enriched, adding information to it both by hand and with computational tools. The end result is a database of editions, authors publishers and other information found from the ESTC, which is much more suited for quantitative research than the original ESTC data. Why to spend so much time working on data about books, one might ask? To put it simply, a data set like the ESTC allows the studying of all sorts of historical phenomena related to printing from vernacularisation (the process in which the language of power and knowledge shifts to the language spoken by the inhabitants of the country) to political crises, which tend to create upsurges in pamphleteering (Lahti et al. 2019). Because the ESTC is machine readable, it can be used to study these and other questions with the methods of data analysis, network analysis included. ESTC has been used to study all sorts of networks of actors, like authors, publishers and printers (M. J. Hill et al. 2019). A very common type of network constructed from the ESTC data connects actors who have been involved in the production of the same edition, based mostly on the information from the imprint of the book. Interestingly from modern perspective, these networks are not always centered and might even exclude the authors. Publishers had significant influence as gatekeepers of what was being actually printed, analysis of their business arrangements are an important context for phenomenon like the Scottish Enlightenment (Sher 2006). One option for the data of the final project is a subset of the ESTC, from which a network of publishers/editions/authors or other entities - like networks of editions - is provided by the course organisers. Tweetsets and hydrator Another option for data of the final project is Twitter, and getting the data will be the exercise related to this lesson. Note that there is no additional work in choosing Twitter data, as everyone will acquire a Twitter data set whether they are going to use it or not. Twitter data has been studied with network analysis from many perspectives, from the spread of false information (Shahi, Dirkson, and Majchrzak 2021) to the spread of news about the discovery of Higg’s boson (Domenico et al. 2013). If you are interested of contemporary questions about culture, politics or internet communities, there most likely a (network) perspective to Twitter that you can work on. Networks can be made, e.g. of the quoting, retweeting, following, mentioning or other interaction between users. However, getting data from Twitter on your own could be non-trivial after only a week’s introduction to R, and there are many other caveats as well. Because of Twitter’s user policy, only tweet id’s can be shared publicly. This means that you will not find Twitter data suitable from network analysis from openly available academic repositories of data. Collecting your own private data set also means plenty of data collection, which would take much time if started from scratch. Fortunately, there is a service that allows you to get tweet ids related to your topic of interest with an user interface (no coding needed), and a software that then can attach relevant information about the tweets (like the user id, content etc.) to them automatically (no coding needed here either). You can get a customised data set from Twitter data about your topic of interest without any coding at all and without breaking the rules of the platform. The next section guides you through the steps of doing this with the Service Tweetsets and the software Hydrator. Quoting and retweeting networks can be constructed from the data obtained in this way. Instructions for homework Follow this excellent guide from the programming historian to the point where you have a data set from Twitter downloaded to your computer. Stop when you have the hydrated data at your computer, we do processing part differently from the tutorial: https://programminghistorian.org/en/lessons/beginners-guide-to-twitter-data Some tips to make the process go smoothly: make a Twitter account in advance Do not create a subset of more than some hundreds of thousands of tweets (you can make a larger data set later if you want to, but hydrating the tweets takes some time). The file that is needed to install the Hydrator from Github varies by the operating system of your computer. It is (the part of the file name after .) exe for Windows, deb for ubuntu and dmg for macOS. Save the output from the Hydrator as a csv file. After you have the data set, put it to your working directory in your CSC workspace. There is no coding exercise for this session, but we will use the data set you acquired in the exercises of the next session. References References "],["week-2-class-2-cleaning-the-observed-and-thinking-about-the-unobserved-data.html", "Week 2, Class 2: Cleaning the Observed and Thinking About the Unobserved Data Unharmonised and Missing data, or the Universals of Data Analysis Unharmonised Data - An Overview Harmonisation Table Regular Expressions and the Stringr Tools Other Solutions To Harmonisation of Data Missing data For the Next Week References", " Week 2, Class 2: Cleaning the Observed and Thinking About the Unobserved Data Unharmonised and Missing data, or the Universals of Data Analysis Although network analysis is the topic of this course, it shares many elements with other approaches to research that apply data, statistics and computation. Perhaps most universal of these similarities are the problems that need to be solved before (network) analysis can begin: those of unharmonised and missing data. In the worst case, jumping over or failing the steps to tackle them can lead to completely faulty results or interpretations. Fortunately, there are things you can do to make your data (or, very least, your interpretation of it) better. Unharmonised Data - An Overview Computers do not have an understanding of relevant and irrelevant differences. This means, that even the smallest of inconsistencies in denoting a thing can have big differences in data analysis. These inconsistencies stem from various sources. There were often various ways to write a name of a city or a person during the early modern period, so often even data that has been “accurately” collected ends up being unharmonised. Both modern and pre-modern data are also subject to mistakes that happen when the data is recorded or transformed from one format to another. Regular users of research libraries will often find instances where mistakes have been made in the cataloging process, and the users of Twitter that make the contents of a social network data sets might not respect spelling rules to begin with. If these different variants of the same thing are not accounted for, the data will be inflated by pseudo persons, cities, books or other entities, that should be grouped together. ## [1] &quot;Some name variants of the City of Aachen according to CERL Thesaurus:&quot; ## [1] &quot;Aach&quot; &quot;Achen&quot; &quot;Aix la Chapelle&quot; &quot;Aix-la-Chapelle&quot; This process of matching and standardising different ways of denoting the same things is called data harmonisation. There are various ways of doing data harmonisation. In fact, it is a field of research on its own right (Christen 2012), and many algorithms and tools have been developed to help database developers and users to find and match instances of the same thing. Fortunately, it is not necessary to become an expert in all of these techniques to solve most data harmonisation tasks that you will encounter. Here, we introduce two entry-level tools, that will already solve significant number of harmonisation problems, and keep being useful even if you familiarise yourself with more advanced ones. Harmonisation Table The technically simplest solution is to construct a harmonisation table. The table has at least two columns, one for the unharmonised values and other for corresponding harmonised values. The number unharmonised and harmonised columns can also be greater. The pros of this approach are the technical ease and the possibility of doing such matching that would be hard to automate, like converting variations of latin place names to moden standardised names. You can then join the harmonised values of the variable to your table with the unharmonised values with the join-functions of R. However, this approach does not scale well and always requires manual labor. But for a small data set with intricacies of who maps to what (like is often the case with e.g. historical data) manual approach is sometimes the best choice. ## [1] &quot;A simple harmonisation table&quot; ## name_variants harmonised_name ## 1 Aach Aachen ## 2 Achen Aachen ## 3 Aix la Chapelle Aachen ## 4 Aix-la-Chapelle Aachen Regular Expressions and the Stringr Tools Cleaning Data at Scale Cleaning data often involves making the same kind of operation over many things. For example, a very common source of spelling variation is that at the very end of a string (e.g “City of London.”) there is a white space (“City of London .”), and this same error occurs for tens or even hundreds of entities in our data (“Aachen .”, “Paris .” etc.). A slightly more abstract problem of similar kind would be data that is consistent, but at the wrong level of granularity. If we wanted to analyse families instead of persons, but in our data people were named the following way: ## [1] &quot;Thomas Hobbes&quot; &quot;Maria Hobbes&quot; &quot;John Locke&quot; &quot;Adam Locke&quot; Then we would need to extract the latter part of each name not to count individuals (in this case duplicate variants of the family), and to do this over all names. Fortunately, there is a collection of R tools that allow us to detect textual patterns of both sorts and more than we demonstrated above, and, after finding the patterns, to apply different kinds of operations over them. Using Stringr and Regular Experssions Stringr is a R package (Wickham2022?) that is part of the Tidyverse, the collection of R tools that were introduced last week. It is not the only way to do text pattern detection and manipulation with R, but it being part of a bigger system (that we are already using) and covering the basic tools makes mastering it a good starting point for learning data harmonisation by coding. There are seven main functions in the Str package, and all of them share a component called pattern. The pattern is written as a regular expression. Regular expression is a description of a string of text. It can be as concrete as description of the letter a (the corresponding pattern being “a”), number between 1 and 5 ([1-5]) or a letters-only piece of text at the end of the string ( [a-z]+$) These descriptions follow a certain grammar, that we will discuss more soon. Regular expressions can be used to find spelling variations, data in the wrong format and other things that make data unharmonised for your purposes. The functions of Stringr allow different kinds of operations over the patterns? We consider four of the functions in the String package (for those interested, all of the main 7 are shortly introduced here) that are especially useful for harmonisation and getting to know your data in general. In all cases, the “pattern” parameter is the regular expression, and string is a character vector. str_detect Returns TRUE or FALSE based on whether it found the pattern from the text. Very useful for exploring your data. example &lt;- c(&quot;Thomas Hobbes&quot;,c(&quot;Maria Hobbes&quot;),c(&quot;John Locke&quot;),c(&quot;Adam Locke&quot;)) is_hobbes &lt;- str_detect(example,pattern = &quot;Hobbes&quot;) print(is_hobbes) ## [1] TRUE TRUE FALSE FALSE str_subset Returns those instances where there is match with the pattern. Useful for exploring your data. example &lt;- c(&quot;Thomas Hobbes&quot;,c(&quot;Maria Hobbes&quot;),c(&quot;John Locke&quot;),c(&quot;Adam Locke&quot;)) is_hobbes &lt;- str_subset(example,pattern = &quot;Hobbes&quot;) print(is_hobbes) ## [1] &quot;Thomas Hobbes&quot; &quot;Maria Hobbes&quot; str_replace Replace the pattern with something else. Note that we used a slightly more complicated regular expression, that replaces the letters only part of the string at the end, and that notices that the first letter of the family name is a capital letter. The text that replaces the matched pattern goes to the replacement parameter. example &lt;- c(&quot;Thomas Hobbes&quot;,c(&quot;Maria Hobbes&quot;),c(&quot;John Locke&quot;),c(&quot;Adam Locke&quot;)) family_name_replaced &lt;- example %&gt;% str_replace(.,pattern = &quot; [A-Z]{1}[a-z]+$&quot;,replacement = &quot; insert_family_name_here&quot;) print(family_name_replaced) ## [1] &quot;Thomas insert_family_name_here&quot; &quot;Maria insert_family_name_here&quot; ## [3] &quot;John insert_family_name_here&quot; &quot;Adam insert_family_name_here&quot; str_extract Returns the first match of the pattern. For example, we could use the function to turn the names in our data set to family names. Notice also that we use the str_replace function to remove the whitespace after exracting the family name. example &lt;- c(&quot;Thomas Hobbes&quot;,c(&quot;Maria Hobbes&quot;),c(&quot;John Locke&quot;),c(&quot;Adam Locke&quot;)) family_name_extracted &lt;- example %&gt;% str_extract(.,pattern = &quot; [A-Z]{1}[a-z]+$&quot;) %&gt;% str_replace(.,pattern = &quot; &quot;,replacement = &quot;&quot;) print(family_name_extracted) ## [1] &quot;Hobbes&quot; &quot;Hobbes&quot; &quot;Locke&quot; &quot;Locke&quot; More About Regular Expressions and Stringr in Practice For effective use of String and similar tools, certain familiarity with the language of regular expressions is needed. We don’t aim to teach you all about regular expressions, but enough to get you started. Don’t worry about memorising everything by heart, there is an official cheat cheet. It is more important to get a touch to how regular expressions work. The cheat sheet has both the Stringr and regular expressions basics condensed to it, so it is a good idea to bookmark or download to your computer. There are also more comprehensive Stringr tutorials like this one. Next, we go through some examples that make use of different elements of the “grammar” of regular expressions. It might be a good idea to also look at the str_function examples again after these examples. We use str_replace, because it makes it easy to demonstrate what was the detected pattern (that is then replaced). Trivial patterns # Matching a singe element, the letter T example &lt;- c(&quot;Thomas Hobbes&quot;,c(&quot;Maria Hobbes&quot;),c(&quot;John Locke&quot;),c(&quot;Adam Locke&quot;)) example_replaced &lt;- str_replace(example,pattern = &quot;T&quot;,replacement = &quot; match was here &quot;) print(example_replaced) ## [1] &quot; match was here homas Hobbes&quot; &quot;Maria Hobbes&quot; ## [3] &quot;John Locke&quot; &quot;Adam Locke&quot; # Matching collection of elements, e.g a name example &lt;- c(&quot;Thomas Hobbes&quot;,c(&quot;Maria Hobbes&quot;),c(&quot;John Locke&quot;),c(&quot;Adam Locke&quot;)) example_replaced &lt;- str_replace(example,pattern = &quot;Thomas&quot;,replacement = &quot; match was here &quot;) print(example_replaced) ## [1] &quot; match was here Hobbes&quot; &quot;Maria Hobbes&quot; ## [3] &quot;John Locke&quot; &quot;Adam Locke&quot; Alternates The pattern might allow for different options, which is what the examples in this subsection are about. # Matching either or (one of the two names in this case) example &lt;- c(&quot;Thomas Hobbes&quot;,c(&quot;Maria Hobbes&quot;),c(&quot;John Locke&quot;),c(&quot;Adam Locke&quot;)) example_replaced &lt;- str_replace(example,pattern = &quot;Thomas|Maria&quot;,replacement = &quot; match was here &quot;) print(example_replaced) ## [1] &quot; match was here Hobbes&quot; &quot; match was here Hobbes&quot; ## [3] &quot;John Locke&quot; &quot;Adam Locke&quot; # Matching from a range, here they are numbers from 0 to 9, but they could also be e.g. letters from a to z [a-z]. # In general, the brackets [] allow for alternative matches. example &lt;- c(&quot;Thomas Hobbes 1&quot;,&quot;Maria Hobbes 7&quot;,&quot;John Locke&quot;,&quot;Adam Locke&quot;) example_replaced &lt;- str_replace(example,pattern = &quot;[1-9]&quot;,replacement = &quot; match was here &quot;) print(example_replaced) ## [1] &quot;Thomas Hobbes match was here &quot; &quot;Maria Hobbes match was here &quot; ## [3] &quot;John Locke&quot; &quot;Adam Locke&quot; # Matching from a collection of values, notice that we once again use brackets [] for alternatives, but this time #without range. example &lt;- c(&quot;T&quot;,&quot;M&quot;,&quot;L&quot;) example_replaced &lt;- str_replace(example,pattern = &quot;[TM]&quot;,replacement = &quot; match was here &quot;) print(example_replaced) ## [1] &quot; match was here &quot; &quot; match was here &quot; &quot;L&quot; # The alternatives can also be used to replace anything but what is specified within the []. This happens with ^ example &lt;- c(&quot;A&quot;,&quot;7&quot;) example_replaced &lt;- str_replace(example,pattern = &quot;[^1-9]&quot;,replacement = &quot; match was here &quot;) print(example_replaced) ## [1] &quot; match was here &quot; &quot;7&quot; Anchoring The pattern can be specified to relate to certain parts of the string, namely the start or the end. # The match can be anchored to the start of the string with ^ example &lt;- c(&quot;this is not a number 9&quot;,&quot;7 is a number&quot;) example_replaced &lt;- str_replace(example,pattern = &quot;^[1-9]&quot;,replacement = &quot; match was here &quot;) print(example_replaced) ## [1] &quot;this is not a number 9&quot; &quot; match was here is a number&quot; # The match can be anchored to the end of the string with $ example &lt;- c(&quot;this is not a number 9&quot;,&quot;7 is a number&quot;) example_replaced &lt;- str_replace(example,pattern = &quot;[1-9]$&quot;,replacement = &quot; match was here &quot;) print(example_replaced) ## [1] &quot;this is not a number match was here &quot; ## [2] &quot;7 is a number&quot; Grouping Elements in the pattern can be grouped with parentheses. #Search for either Maria or Thomas, and Hobbes after that example &lt;- c(&quot;Thomas Hobbes&quot;,&quot;Maria Hobbes&quot;,&quot;John Locke&quot;,&quot;Adam Locke&quot;) example_replaced &lt;- str_replace(example,pattern = &quot;(Thomas|Maria) Hobbes&quot;,replacement = &quot; match was here &quot;) print(example_replaced) ## [1] &quot; match was here &quot; &quot; match was here &quot; &quot;John Locke&quot; &quot;Adam Locke&quot; Quantification The pattern can include a specification about the number of times something occurs in the string, this subsection provides examples of that. #Replace Thomas when it occurs from two to three times. example &lt;- c(&quot;Thomas Hobbes&quot;,&quot;Thomas Thomas Hobbes&quot;,&quot;Thomas Thomas Thomas Hobbes&quot;) example_replaced &lt;- str_replace(example,pattern = &quot;(Thomas ){2,3}&quot;,replacement = &quot; match was here &quot;) print(example_replaced) ## [1] &quot;Thomas Hobbes&quot; &quot; match was here Hobbes&quot; &quot; match was here Hobbes&quot; #? * and + can also be used to control how many times something need to occur in a match. Here we demonstrate the use of #+, that means that the pattern needs to be found at least once, but possibly more times. example &lt;- c(&quot;Thomas Hobbes&quot;,&quot;Thomas Thomas Hobbes&quot;,&quot;Thomas Thomas Thomas Hobbes&quot;) example_replaced &lt;- str_replace(example,pattern = &quot;(Thomas )+&quot;,replacement = &quot; match was here &quot;) print(example_replaced) ## [1] &quot; match was here Hobbes&quot; &quot; match was here Hobbes&quot; &quot; match was here Hobbes&quot; Look arounds # Match can be conditioned to the surrounding elements. Here, the regular expression looks for Maria followed by Hobbes. #More ways to do this on the cheat sheet. example &lt;- c(&quot;Thomas Hobbes&quot;,&quot;Maria Hobbes&quot;,&quot;John Locke&quot;,&quot;Adam Locke&quot;) example_replaced &lt;- str_replace(example,pattern = &quot;Maria(?= Hobbes)&quot;,replacement = &quot; match was here &quot;) print(example_replaced) ## [1] &quot;Thomas Hobbes&quot; &quot; match was here Hobbes&quot; ## [3] &quot;John Locke&quot; &quot;Adam Locke&quot; Character matching There are some characters or character combinations with special meaning in regular expressions. We demonstrate some examples, the cheat sheet has a more comprehensive list. # . matches any character except a new line. Here, we use it with + to allow any string without new lines to be a match example &lt;- c(&quot;Thomas Hobbes&quot;,&quot;Maria Hobbes&quot;,&quot;John Locke&quot;,&quot;Adam Locke&quot;) example_replaced &lt;- str_replace(example,pattern = &quot;.+&quot;,replacement = &quot; match was here &quot;) print(example_replaced) ## [1] &quot; match was here &quot; &quot; match was here &quot; &quot; match was here &quot; &quot; match was here &quot; # . is an example of a character, that needs to be written as \\\\., if the aims is to match a dot and not reference to the special #meaning, in this case any character, of that character in regular expressions. The same applies if one is searching e.g for [ or (. example &lt;- c(&quot;Thomas Hobbes.&quot;,&quot;Maria Hobbes&quot;,&quot;John Locke&quot;,&quot;Adam Locke&quot;) example_replaced &lt;- str_replace(example,pattern = &quot;\\\\.&quot;,replacement = &quot; match was here &quot;) print(example_replaced) ## [1] &quot;Thomas Hobbes match was here &quot; &quot;Maria Hobbes&quot; ## [3] &quot;John Locke&quot; &quot;Adam Locke&quot; # [:digit:] is an example of command that matches a distinct set of characters, in this case digits. example &lt;- c(&quot;Thomas Hobbes 12&quot;,&quot;Maria Hobbes&quot;,&quot;John Locke 7&quot;,&quot;Adam Locke&quot;) example_replaced &lt;- str_replace(example,pattern = &quot;[:digit:]&quot;,replacement = &quot; match was here &quot;) print(example_replaced) ## [1] &quot;Thomas Hobbes match was here 2&quot; &quot;Maria Hobbes&quot; ## [3] &quot;John Locke match was here &quot; &quot;Adam Locke&quot; Combining elements Some of the examples have already used many of the properties of regular expressions at once, and this is often extremely helpful, as it makes regular expressions much more expressive. The last example uses many of the things we have dicussed at the same time. # Task, match the people who were born at the 17th century and have a de at their names example &lt;- c(&quot;Baruch de Spinoza 1632-1677&quot;,&quot;John Locke 1632-1704&quot;,&quot;François Fénelon 1651-1715&quot;) example_replaced &lt;- str_replace(example,pattern = &quot;.* de .* 16[0-9]{2}-1[0-9]{3}&quot;,replacement = &quot; match was here &quot;) print(example_replaced) ## [1] &quot; match was here &quot; &quot;John Locke 1632-1704&quot; ## [3] &quot;François Fénelon 1651-1715&quot; Here we first allowed string of any length (first name), then we required de, then allowed for another string of any length or type, after which we required 16 followed by two other numbers (birth year in the 17th century), followed by - and the year of death. And this is by no means as expressive as a regular expression can get. Regular Expressions and Stringr - a Summary Now, you should have a some sort of understanding in how regular expressions can be used in the manipulation of strings. We will demonstrate the practical usefulness of these tools in the exercise set of this week, where we will use some of these tools to clean the Twitter data set you obtained last time. Other Solutions To Harmonisation of Data It should also be mentioned, that data harmonisation can be done without building a code workflow. Openrefine is an example of a harmonisation tool with an user interface. We do not prohibit the use of such tools for e.g. harmonising your data set for the final project, and it can a reasonable choice in many instances. However, it will serve you in the long run to get familiar at least with regular expressions, and possibly more. Building workflows of data harmonisation often takes as much or more time than the actual analysis in DH research, so building competence in these skills is useful. Missing data Another major theme is data that is lacking. It is very common in the humanities and the social sciences, that some of the data that should be part of the data set is not actually there. This phenomenon comes in two major forms, as the data can be either completely or partially missing. Here, we take a quick overview to these two types of missingness, and to some of the ways the problems caused by them can be alleviated (or at least understood). Partially missing data is a known unknown, you can observe just by looking at the data set. In R, you might have seen value NA in a cell of a table, where you would have expected a character or numeric value. In other words, partially missing data refers to instances where we are missing some of the values of variables for an observation. The reasons for missing data vary. People might not answer all the questions in a survey, the piece of data might be unavailable (e.g. books imprint might lack the name of the publisher), query used to get the data might miss values in the wrong format, or any other reason. The end result is still the same: a collection of missing elements in your data. ## name occupation ## 1 Thomas Hobbes philosopher ## 2 Steven Steel &lt;NA&gt; There are two reasons why the partially missing data matters. One is purely technical: many operations can not be performed over missing values. For example, lets assume that you want to take the average number of pages for three books, but for one of the books, the number of pages is missing. As “missing” or NA is not a number, you can’t sum it with numbers, hence you can not take the average. Most of data analysis in the humanities and the social sciences would be impossible if problems like this stopped as from taking averages or from doing other sorts of analyses that can’t tolerate missing values. One common solution is simply to ignore the missing values as if they did not exist in the data. In the example, this would mean that the average is taken over the number of pages of the two books with page number information available. The other reason for why partially missing data is problematic, is that missing values might not be missing at random. For example, think of a survey that asks peoples opinions about a polarising topic: people who are not answering might do this because they think their opinion is controversial, and they don’t trust the anonymity of the survey. This is one of the reasons why the simplest solution - just ignoring the missing values, more formally omission - is sometimes dangerous. Other solutions include imputation (filling of missing information with an estimate) and analyses that are designed to be less affected by missing data. We do not teach imputation or analyses robust to missing data on this course, but we emphasize that they are motivated by a question anyone analysing their data should consider: are the missing values Missing Completely at Random (MCAR), or does data missing correlate with the properties of the data? If yes, is it possible that this affects the results of a (network) analysis done with the data? There is no out-of-the-box solutions to these problems, which makes (among other reasons) the expertise that scholars from the humanities have about their subject very valuable to data analysis as well, as they often know about such potential pitfalls in the sources they are working with (even if they don’t articulate these things in terms of statistics). Completely missing data makes the unknown unknowns related to your data, those instances that are not there at all. Like partially missing data, completely missing data can have various causes. For example, some information in social media can be hidden, meaning that queries or scraping will miss it. Historians, archeologist and others working with pre-modern societies face missing data all the time, as many books, letters and other artifacts known to have existed have been completely lost. Completely missing data does not create the same kind of technical problems as partially missing data, as they are not present in your database in any form. However, their implications for the meaningfulness and interpretation of, lets say, your network, can be as or even more severe than those cases that are partially missing. Entire observations can also be missing at random, or it might be related to the properties of the observations themselves. For example, there is strong evidence that the ESTC is heavily biased towards certain kinds of items before 1641, covering them extensively (e.g. bibles) while missing most examples of other types of items (A. Hill 2016). In some instances, it is possible to evaluate how much and what is missing, like with the ESTC, that can be compared to other sources. However, this is not always the case. Fortunately, it is always possible to think what your (network) data needs to be to serve your research question. For example, is it necessary that your network covers all the persons and connections that could be involved, or is a good proportion or a sample (representative set of data from the population of your interest) enough? Do the relevant results change radically if you drop some of the network edges away at random to test how sensitive the results are to some variation of the data set? If so, is it possible that your results are very sensitive to missing data as well. Very often, the research process and closer analysis of the data reveals that the original question required too much from the data, or that it is much more suited to analyse something else that was revealed during the research process. But this is a normal part of research, and should not be feared. For the Next Week The treatment of missing data can lead to dramatic changes in results. The article to read for the next week is a critique of another article, published in a very prestigious journal. You should be able to access the article through Helka. The article is not the easiest, but try to understand the main ideas. We will discuss the article next week. Especially, everyone should be prepared to answer these four questions in front of the class. Who answers will be selected randomly during the lecture (one person per question, no more than one question per student). The questions are: Q: What is the forward bias in the Seshat data according to the authors? Q: How the missing information about moralising gods was handled in the R script of the original article? Q: How the predicted emergence of moralising gods differ between the revised regressions and the original Seshat data set? Q: What has happened to the original article? References References "],["week-3-class-1-data-modelling.html", "Week 3, Class 1: Data modelling", " Week 3, Class 1: Data modelling This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. Try executing this chunk by clicking the Run button within the chunk or by placing your cursor inside it and pressing Cmd+Shift+Enter. plot(cars) Add a new chunk by clicking the Insert Chunk button on the toolbar or by pressing Cmd+Option+I. When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the Preview button or press Cmd+Shift+K to preview the HTML file). The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike Knit, Preview does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed. "],["week-3-class-2-build-a-data-model-with-r.html", "Week 3, Class 2: Build a Data Model with R", " Week 3, Class 2: Build a Data Model with R This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. Try executing this chunk by clicking the Run button within the chunk or by placing your cursor inside it and pressing Cmd+Shift+Enter. plot(cars) Add a new chunk by clicking the Insert Chunk button on the toolbar or by pressing Cmd+Option+I. When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the Preview button or press Cmd+Shift+K to preview the HTML file). The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike Knit, Preview does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed. "],["week-4-class-1-network-concepts-and-metrics.html", "Week 4, Class 1: Network Concepts and Metrics Exercises Introduction Social network concepts: triadic closure, transitivity, brokerage Node-level metrics Edge-level metrics Global Metrics Conclusions", " Week 4, Class 1: Network Concepts and Metrics Exercises This lesson has a corresponding editable notebook containing some exercises. These are stored as a separate notebook in the CSC notebook workspace, with the name 4-1-fundamentals_nb.Rmd. You’ll need to move a copy to your ‘my-work’ folder in your CSC notebook Workspace, complete the exercises, and knit them, using the method we learned in the first class. Once this is done, send a copy of the HTML file to the course leaders. Introduction This lesson will cover a number of things. First, it talks through some further conceptual things relating specifically to social networks, and second, introduces a large number of measurements by which we can understand more about a given network. These are generally divided into node-level and global metrics. Many node-level metrics are interested in centrality, that is, working out the most important and most influential nodes in a network. In a social network, this may point to individuals with a particular influence or role in the network, but it can also have other uses: Google’s PageRank algorithm uses a version of centrality to work out which web pages are likely of the highest quality or usefulness for a particular search, for example. Social network concepts: triadic closure, transitivity, brokerage Triadic closure Triadic closure is the simple idea that people in a social network are more likely to connect to each other if they share another common connection. In social network theory, this is very common, and thought to drive much of the process of network formation. Undirected Network Figure 4: Four possible triad types in an undirected network Directed Network The 16 possible triad combinations in a directed network Node-level metrics Degree The degree of a node is a count of its connections. Degree can be weighted and directed. In a directed network, the separate degree counts are called in-degree and out-degree. The total degree is the sum of the two. In this graph, the ego is connected to each alter by an incoming and outgoing link. Weights can be added to the links: degree is a sum of the weights of the links, in that case. The edge weights are labelled in the diagram below, and the numbers inside each node give the in and out degree measurements. Depending on the network, these in and out-degree measurements may signify different types of nodes. For example, imagine a network of Twitter accounts: each follower is counted as an incoming link, and each person you follow is counted as an outgoing link, for the purposes of your degree score. If you have a high in-degree, you might have a high influence in this network, because you can broadcast ideas or whatever to a large number of followers. Having only a high out-degree in this network, on the other hand, may not make you very influential at all, because it means you follow many accounts but few follow you back. Degree is the most common measurement of network centrality, or importance. A node with a high degree is likely to have more influence in the network, to have more paths pass through it, and so forth.. However, it doesn’t always point to the most ‘important’ nodes. First, degree measures quantity over quality of links. The node with the highest absolute number of links will always be ranked highest, no matter which nodes those links point to. Degree distribution Degree is used to measure structural properties of the network. We can do this using the degree distribution. A distribution counts how many nodes have a degree of 1, a degree of two, three, and so forth, or sometimes a range, such as a degree of between 1 and 5, 6 and 10, and so on. As mentioned in a previous chapter, this degree distribution tells us some valuable structural properties about the network, such as the extent to which it is a collection of hubs and less important nodes (known as ‘scale-free’). To illustrate the degree distribution, below is a histogram of the degree distribution for a randomly-created scale-free network with 1000 nodes and 1000 edges. It shows that 0 to 10 nodes have a degree score of 50, 11 - 20 have a degree score of 4, and at the other end, more than 400 nodes have a degree score of just 1. Another way to illustrate this is with a log-log plot. A log-log plot shows the same data, except the scales are logarithmic, making them easier to read. Networks with distributions which look like this on a histogram or log-log plot are likely to be scale-free, meaning their structure consists of a number of important hubs with high degree, followed by a larger number of less-important nodes. Network science has shown that these network types are very common in all sorts of real-world situations, from biological, to social, to computer networks. They’ve also been shown to have a number of particular properties: for example, it may make a computer network resistant to random ‘attack’, because its only when hubs are removed that the network begins to become disconnected. Removing unimportant nodes doesn’t affect the overall system. At the same time, this can make them vulnerable, if hub nodes are specifically targeted. Betweenness Centrality Betweenness centrality measures all the paths between every pair of nodes in a network. A node has a high betweenness centrality if it is used as a ‘hop’ between many of these paths. In the below diagram, A has a higher betweenness centrality than B, because it is needed to traverse from the left to the right side of the network. Betweenness centrality for a given node v is calculated by measuring the shortest path between every pair of nodes, and then counting the total number of these paths which pass through v. It can be directed, meaning that paths are only considered if they exist in the correct direction. As mentioned above, degree is not always the best way to measure the importance of a node. It can also be important to consider the node’s particular position, and the role it occupies within a larger system. In the above diagram, node A is the only way through which any of the nodes on the left can reach any on the right. In a social network, nodes with high betweenness are often thought of as important ‘brokers’, as we talked about in a previous lesson. However in practice, note that high degree nodes are often the ones which have the highest betweenness values too, and there are often multiple paths between the same pair of nodes, which can limit the use of this metric. Eigenvector Centrality A node that has a high eigenvector score is one that is adjacent to nodes that are themselves high scorers. Google’s original PageRank algorithm used a version of this concept: ranking web pages higher in search results if they themselves were linked to by other high-scoring pages. In the example network above, both A and B have two incoming connections each, but A has a higher eigenvector centrality score because it is connected to more well-connected nodes. Closeness centrality Closeness centrality measures each node’s path to every other node in the network: a node is declared more central if its paths to other nodes tend to be short by this measurement (the formula is 1 divided by the average of all the shortest paths from a given node to all other nodes). Closeness centrality might be thought of as a way of measuring how quickly something flowing through the network will arrive. In the diagram below, A has a higher closeness centrality than B, even though both have the same number of connections, because A is ‘less far’ to all other nodes, on average. Edge-level metrics Global Metrics Density The density of a network is defined as the fraction of edges which are present out of the total possible number of connections. Consider the undirected network below (a), which has four edges in total. The ‘full graph’ (when every node is connected to every other) of an undirected network with four nodes has six edges (b). The density of (a) therefore is 4/6, or .6666… In practice, most real-world networks are much less dense. What implication does this have on the ‘small world’ networks we talked about in an earlier class? The ‘small world’ effect works in low-density networks precisely because of the existence of hubs and weak ties. If there are very few ties in a network altogether, but there are a number of very well-connected hubs, then most nodes will be able to reach any other node by going through these hubs. In a directed network, the possible number of nodes in the full graph is doubled. On its own, density may not tell us much about the structure of a graph. Two graphs with 20 edges and 10 nodes may look very different: in one, each node may have two connections, and in another, 9 nodes may have one connection each, and the final one 10 connections. However, in combination with some other metrics, most importantly degree distribution, it may be a clue as to the underlying structure of the network. Average path length The average path length is the average distance in ‘hops’ from one node to another in the network. In the Milgram experiments, the average path length through the network was found to be between 5 and 6. This measurement, therefore, can tell us how long it may take on average for information to get from one side of a network to another. A network with a small average path length is said to be a ‘small world’. Consider a letter network with an average path length of 4. Does this mean that information needs to go through on average 2 further people (because the start and end nodes are counted in the path) to get from A to B? Well, not necessarily. This is a good example of why we should consider each network and set of connections carefully before using these metrics. Common sense tells us that in reality, if person A really wanted to communicate with B, there may have been no reason why they couldn’t send a letter directly, without using the ‘shortest path’ through the other two nodes. Clustering coefficient The clustering coefficient is also known as transitivity, and it is defined as the ratio of completed triangles and connected triples in the graph. The more completed triangles a network contains compared to the overall triples, the more clustered it is said to be. Clustering is a complex topic which forms a large part of the field of Social Network Analysis. On a local level, transitivity means the probability node B will be connected to node C, if both nodes are connected to a common neighbour, A. The higher this probability, the more the graph is said to display clustering tendencies: if you are more likely to connect to your common neighbours, it follows that it is more likely that the graph will be divided into dense cliques and sub-groups, rather than a free-for-all where everyone is connected to everyone else. Groups and Community Structure in Networks Components Cliques Networks and communities are often thought of as analogous, but in network analysis, the latter are considered a subset of the former. Discovering discrete communities—often called sub-graphs—within a wider network has long been one of the fundamental applications of network science. In epidemiological networks, for example, the extent to which a network tends to break into sub-graphs and the constitution of those parts has important implications for understanding the spread of disease through a system. While the ego networks of the previous chapters are communities in their own right,  working with a large set of merged, overlapping ego networks invites us to think about the whole, and how this whole might be redivided into clusters not necessarily corresponding to the ego networks which constitute its parts. In a network sense, a community might be generally defined as a sub-grouping within a network which is in some way more densely connected to its internal group members than to those outside the group. The problem, of course, is that if the graph is connected (i.e. if all nodes can reach all other nodes eventually), then the boundaries of any detected community that goes beyond a simple ego network are necessarily arbitrary, flexible, and subjective. It is true that many individuals will sit firmly in the core of a given cluster, but there will be those on the margins who might be considered as members of more than one community—or none.  In spite of these problems, this rigid way of thinking about communities can serve as a useful tool, to which we can bring our existing knowledge of historical communities and compare them to an algorithm’s results. By forcing us to over-simplify, they may not be able to capture the full extent of multiple overlapping communities of identity, but they do help us to reflect on how we think of individuals operating within communities of correspondence. For example, though we might think of person X primarily as a member of group Y, if we look just at their patterns of correspondence, they seem more firmly in group Z. This is not to say that we should necessarily re-evaluate the scholarship and declare that they were in group Z all along, but it might be an indication that we should re-assess that person’s patterns of correspondence—after all, belonging to a community and being more likely to correspond with its members are not the same thing.  Community Detection Methods Network science has a large of algorithsm for finding optimal communities in graphs. Many rely on a metric called modularity, explained in this next section. Modularity Modularity is a score evaluating how well a given set of group labels describe the groups found in a network. It’s calculated as the number of edges falling within groups minus the expected number in an equivalent network with edges placed at random (link). In the below diagram, the modularity is high, because the groups (coloured as either red or green nodes) correctly describe the two separate ‘communities’ found in the network. Community detection algorithms work by trying to maximise this value. As with the clustering coefficient, a high modularity indicates the extent to which the network might be described as a collection of densely-connected sub-groups, or whether the links are spread out evenly across the network. Figure 5: A network with high modularity. The given partitions (separate colours) successfully divide the network into groups with many more connections between the groups than to other groups. Louvain Community Detection The Louvain algorithm is one algorithm which uses modularity, developed specifically for use in large graphs. At first, each node in the network is assigned to a community of just itself. Then, one at a time, each node is temporarily moved to the community of each of its neighbours, and the modularity of the whole graph is re-checked using the method above. Each node is kept in the community which results in the highest change in overall modularity.  At this point, most nodes are now in a community of two (if there’s no change in modularity, they stay in their own community). The next step treats these mini-communities as nodes in a network in their own right, and runs the same process again (putting each mini-community into the mini-community of its neighbour, calculating overall modularity, and moving to that which gives it the highest gain). This whole process then repeats until modularity stops increasing and communities stop merging with other communities—or a pre-supplied threshold is reached. In very large graphs, the process is more difficult and likely to give different results depending on the random starting point. Also, while the method may be a useful way of finding communities without preconceived starting points, it does require us to think of the network as being divided into communities of some kind. Every node must be placed in a community - none can be placed outside the communities, and, perhaps more controversially, with this method, no node can be a member of more than one community.  From Traag, V.A., Waltman, L. &amp; van Eck, N.J. From Louvain to Leiden: guaranteeing well-connected communities. Sci Rep 9, 5233 (2019). https://doi.org/10.1038/s41598-019-41695-z Important to note is that communities are to some degree subjective, and ‘densely-connected’ can have different meanings depending on the algorithm. And needless to say, detecting communities on a graph alone does not make a research question! Conclusions This tour of network metrics has just introduced some of the more common ways a network can be measured. All of these metrics can be applied to any network, but bear in mind that many have been developed with the specific aim of measuring relations between people in some sort of set of social interactions, and the conclusions drawn are based on the consideration that the individuals are in some sense ‘free agents’, and can freely connect with other nodes as they wish. In many other network types (books, or cities for example), there may be hidden factors and other constraints guiding the structure of the network. This is not to say that the metrics cannot be used, but just that the conclusions from them may not be the same as those in SNA literature. "],["week-4-class-2-network-analysis-with-r.html", "Week 4, Class 2: Network Analysis with R Introduction Network data structures Creating a Network Object in R from an Edge List Calculating Network Metrics Joining additional data Transitivity, Triads, structural balance Group detection Conclusions", " Week 4, Class 2: Network Analysis with R Introduction In this class, we’ll talk through creating and analysing a network object in R. The network we’ll work with is a sample of correspondence data taken from the British State Papers in the seventeenth century. Network data structures Edge lists Adjacency Matrices Creating a Network Object in R from an Edge List One of the easiest data formats to construct a network is an edge list: a simple dataframe with two columns, representing the connections between two nodes, one per row. It makes particular sense with correspondence data, which is often stored as records of letters with a ‘from’ and a ‘to’—more or less a ready-made edge list. In a correspondence dataset you might also have multiple sets of each of the edges (multiple letters between the same pair of individuals). This will be added to the edges as a ‘weight’. We will use three R network libraries to do almost everything network-related, from analysis to visualisation: igraph, tidygraph and ggraph. The goal is to port everything to a format which is easy to work with using existing an established data analysis workflow. That format is known as ‘tidy data’, and it is a way of working with data which is easily transferable across a range of uses. It also means you need to learn very little new programming to do network analysis if you stay within this ‘ecosystem’. Import Network Data The workflow uses a number of R packages. In the CSC Notebooks environment, these have already been installed and can be loaded using the commands below. If you are doing this on a local machine, you may have to install them first using the command install.packages(), with the package name specified as a string, for example install.packages('igraph'). library(tidyverse) library(igraph) library(tidygraph) In this class, we’ll use a dataset derived from the English State Papers. It takes the form of a .csv containing the information on the author, recipient, and date of sending for a small sample of state letters sent between 1670 and 1672. Read the file into R with the read_csv() function from a previous lesson: letters = read_csv(&quot;letter_data.csv&quot;, col_types = cols(.default = &quot;c&quot;)) The letters dataset is a simple dataframe. Each row represents a letter record, and has a unique ID. Essentially, each row is a record of who the author and recipient of the letter. Each of these senders and recipients also have both a unique ID, and the original name of the letter writer and sender. The unique ID is used because it’s quite likely that the names are not unique, and the network could combine two nodes with the same name together, for example. This is used to construct an edge list. If you have multiple letters between individuals, you can count them and use as a weight in the network, or you can ignore it. This is done with tidyverse commands we learned previously: group_by() and tally(), changing the name of the new column to ‘weight’. edge_list = letters %&gt;% group_by(from_id, to_id) %&gt;% tally(name = &#39;weight&#39;) edge_list ## # A tibble: 364 × 3 ## # Groups: from_id [198] ## from_id to_id weight ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 E000145-S012650-T000000 E022443-S042999-T000000 1 ## 2 E000189-S014115-T000000 E004830-S019312-T000000 4 ## 3 E000189-S014115-T000000 E006871-S022202-T000000 1 ## 4 E000189-S014115-T000000 E022443-S042999-T000000 1 ## 5 E000312-S012741-T000000 E022443-S042999-T000000 2 ## 6 E000393-S036175-T000000 E001993-S018912-T000000 1 ## 7 E000520-S001769-T000000 E003476-S051213-T000000 1 ## 8 E000520-S001769-T000000 E022443-S042999-T000000 1 ## 9 E000799-S042396-T000000 E022443-S042999-T000000 1 ## 10 E001346-S047404-T000000 E022443-S042999-T000000 1 ## # … with 354 more rows Now you see each unique combination of sender and recipient. If there are multiple letters, this is now signified by a weight of more than one in the weight column. You’ll also notice that the other information (letter IDs and actual names) has disappeared. This is not needed to make the network, but we can bring the name information back later. Turn the edge list into a tbl_graph Next transform the edge list into a network object called a tbl_graph, using tidygraph. A tbl_graph is a graph object which can be manipulated using tidyverse grammar. This means you can create a network and then use a range of standard data analysis functions on it as needed. Use as_tbl_graph() to turn the edge list into a network. The first two columns will be taken as the from and to data, and any additional columns added as attributes. An important option is the directed = argument. This will specify whether the network is directed (the path goes from the first column to the second) or undirected. Because this network is inherently directed (a letter is sent from one person to another), we use directed = TRUE. In many cases, the network will be undirected, and this should be specified using directed = FALSE. sample_tbl_graph = edge_list %&gt;% as_tbl_graph(directed = T) sample_tbl_graph ## # A tbl_graph: 248 nodes and 364 edges ## # ## # A directed simple graph with 6 components ## # ## # Node Data: 248 × 1 (active) ## name ## &lt;chr&gt; ## 1 E000145-S012650-T000000 ## 2 E000189-S014115-T000000 ## 3 E000312-S012741-T000000 ## 4 E000393-S036175-T000000 ## 5 E000520-S001769-T000000 ## 6 E000799-S042396-T000000 ## # … with 242 more rows ## # ## # Edge Data: 364 × 3 ## from to weight ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 89 1 ## 2 2 22 4 ## 3 2 199 1 ## # … with 361 more rows The tbl_graph is an object containing two linked tables, one for the edges and one for the nodes. You can access each of the tables using the function activate(nodes) or activate(edges). The active table is listed first and has the word ‘active’ in the description. sample_tbl_graph %&gt;% activate(edges) ## # A tbl_graph: 248 nodes and 364 edges ## # ## # A directed simple graph with 6 components ## # ## # Edge Data: 364 × 3 (active) ## from to weight ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 89 1 ## 2 2 22 4 ## 3 2 199 1 ## 4 2 89 1 ## 5 3 89 2 ## 6 4 10 1 ## # … with 358 more rows ## # ## # Node Data: 248 × 1 ## name ## &lt;chr&gt; ## 1 E000145-S012650-T000000 ## 2 E000189-S014115-T000000 ## 3 E000312-S012741-T000000 ## # … with 245 more rows You can use many of the tidyverse commands we learned in the earlier lesson on this object, for example filtering to include only edges with a weight of more than 1: sample_tbl_graph %&gt;% activate(edges) %&gt;% filter(weight&gt;1) ## # A tbl_graph: 248 nodes and 165 edges ## # ## # A directed simple graph with 121 components ## # ## # Edge Data: 165 × 3 (active) ## from to weight ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2 22 4 ## 2 3 89 2 ## 3 9 74 2 ## 4 10 89 6 ## 5 12 154 2 ## 6 13 202 2 ## # … with 159 more rows ## # ## # Node Data: 248 × 1 ## name ## &lt;chr&gt; ## 1 E000145-S012650-T000000 ## 2 E000189-S014115-T000000 ## 3 E000312-S012741-T000000 ## # … with 245 more rows Calculating Network Metrics Global metrics The first thing we want to do with this network is to calculate some global network statistics. Because the outputs to these are generally a single number, we don’t need to worry about storing them in a table, as we’ll do with the node-level metrics later. To calculate these metrics, generally just pass the network to a relevant function. These metrics were covered in more detail in the previous class. Density (the number of links present out of all possible links): sample_tbl_graph %&gt;% igraph::graph.density() ## [1] 0.005942275 Average path length (the average number of hops between every pair of nodes in the network): sample_tbl_graph %&gt;% igraph::average.path.length() ## [1] 11.04997 Clustering coefficient: Because there are a number of ways to calculate clustering in a network, a method needs to be specified. The clustering coefficient is also known as transitivity, and it is defined as the ratio of completed triangles and connected triples in the graph. This measurement can be global (which counts the overall ratio) or local (which counts the individual ratio for each node). Because we want the global measurement, specific this with the type = argument. sample_tbl_graph %&gt;% igraph::transitivity(type = &#39;global&#39;) ## [1] 0.02453653 Node-level metrics. There are a number of ways to calculate node-level metrics (these are things like degree, betweenness as explained in the previous class). For example, you can use igraph functions to calculate the degree of single node or group of nodes. The following code returns the degree for the node with the ID E004654-S006979-T000000 (King Charles II of England). The argument mode = specifies the type of degree: in, out, or all, as we learned in the previous lesson. sample_tbl_graph %&gt;% igraph::degree(v = &#39;E004654-S006979-T000000&#39;, mode = &#39;all&#39;) ## E004654-S006979-T000000 ## 52 For most purposes, a table containing each of the nodes and the relevant metrics would be more useful (and something that can be analysed using R later). To do this, we use a function called mutate(). Mutate creates a new column containing the value from some calculation, which is performed on each row in the dataset. Assign the name degree to the new column with degree =. The column should contain the total degree score for each node. This is done using the function centrality_degree(). With the two additional arguments in this function, specify the mode (in, out, or all) and, if a weighted degree score is desired, the column to be used as weights. sample_tbl_graph %&gt;% activate(nodes) %&gt;% # make sure the nodes table is active mutate(degree = centrality_degree(mode = &#39;all&#39;, weights = weight)) ## # A tbl_graph: 248 nodes and 364 edges ## # ## # A directed simple graph with 6 components ## # ## # Node Data: 248 × 2 (active) ## name degree ## &lt;chr&gt; &lt;dbl&gt; ## 1 E000145-S012650-T000000 1 ## 2 E000189-S014115-T000000 21 ## 3 E000312-S012741-T000000 6 ## 4 E000393-S036175-T000000 3 ## 5 E000520-S001769-T000000 2 ## 6 E000799-S042396-T000000 1 ## # … with 242 more rows ## # ## # Edge Data: 364 × 3 ## from to weight ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 89 1 ## 2 2 22 4 ## 3 2 199 1 ## # … with 361 more rows The data format allows you to use dplyr pipes %&gt;% to perform one calculation on the data, then pass that new dataframe along to the next function. Here we calculate the degree scores first, then filter to include only nodes with a degree score over two: sample_tbl_graph %&gt;% activate(nodes) %&gt;% mutate(degree = centrality_degree(mode = &#39;total&#39;)) %&gt;% filter(degree &gt;2) ## # A tbl_graph: 51 nodes and 124 edges ## # ## # A directed simple graph with 2 components ## # ## # Node Data: 51 × 2 (active) ## name degree ## &lt;chr&gt; &lt;dbl&gt; ## 1 E000189-S014115-T000000 8 ## 2 E001993-S018912-T000000 37 ## 3 E002622-S014197-T000000 10 ## 4 E003476-S051213-T000000 6 ## 5 E003494 3 ## 6 E004454-S051301-T000000 3 ## # … with 45 more rows ## # ## # Edge Data: 124 × 3 ## from to weight ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 8 4 ## 2 1 29 1 ## 3 2 7 1 ## # … with 121 more rows Summarising the network data To work with your new network metrics, the data can be outputted to a standard R dataframe. Create a new dataframe by doing this, using the tidyverse function for creating dataframes, as_tibble(): network_metrics_df = sample_tbl_graph %&gt;% activate(nodes) %&gt;% # make sure correct table is active mutate(degree = centrality_degree(weights = weight, mode = &#39;all&#39;)) %&gt;% # calculate degree mutate(between = centrality_betweenness(weights = weight,directed = F)) %&gt;% # calculate betweenness centrality as_tibble() # turn the nodes table into a plain dataframe ## Warning in betweenness(graph = graph, v = V(graph), directed = directed, : ## &#39;nobigint&#39; is deprecated since igraph 1.3 and will be removed in igraph 1.4 This new table can be sorted, totals counted, and so forth: network_metrics_df %&gt;% arrange(desc(degree)) ## # A tibble: 248 × 3 ## name degree between ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 E004830-S019312-T000000 604 8048. ## 2 E022443-S042999-T000000 270 13404. ## 3 E600059-S012260-T000000 161 3223. ## 4 E013473 150 235 ## 5 E004654-S006979-T000000 91 11774. ## 6 E001993-S018912-T000000 72 6663. ## 7 E903376-S024822-T000000 57 1623 ## 8 E008063 51 0 ## 9 E002620-S051125-T000000 47 0 ## 10 E906453-S026312-T000000 47 0 ## # … with 238 more rows Joining additional data The value of working with a data model and tidygraph is that we can merge additional tables of data attributes to our nodes or edges. In a separate table, we have a dataset of attributes about this set of nodes, including place and dates of birth and death, and gender. Using the join() commands, we can merge this table to the network data, and use it to sort, filter (and later visualise) the data: First, load the table of data using read_csv: node_attributes = read_csv(&#39;node_attributes.csv&#39;) ## Rows: 248 Columns: 13 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (11): name, main_name, all_names, links, gender, roles_titles, wikidata_... ## dbl (2): birth_year, death_year ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. node_attributes ## # A tibble: 248 × 13 ## name main_…¹ all_n…² links birth…³ death…⁴ gender roles…⁵ wikid…⁶ occup…⁷ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 E006019… Dr. Ra… Dr. Ra… http… 1617 1688 male theolo… http:/… theolo… ## 2 E011771… Hyde, … Edward… http… 1609 1674 male judge;… http:/… judge;… ## 3 E015103… Sir Ph… Sir Ph… http… 1607 1678 male politi… http:/… politi… ## 4 E020134… Taylor… Silas … http… 1624 1678 male compos… http:/… compos… ## 5 E014743… Sir Ed… Sir Ed… http… 1608 1654 male econom… http:/… econom… ## 6 E020485… Sir Th… Sir Th… http… 1612 1685 male naval … http:/… naval … ## 7 E004654… Charle… Charle… http… 1630 1685 male sovere… http:/… sovere… ## 8 E007992… Heneag… Heneag… http… 1628 1689 male diplom… http:/… diplom… ## 9 E007902… John F… John F… http… 1625 1686 male priest http:/… priest ## 10 S035587 Philad… Philad… http… 1612 1665 female politi… http:/… politi… ## # … with 238 more rows, 3 more variables: place_of_birth &lt;chr&gt;, ## # place_of_death &lt;chr&gt;, politician &lt;chr&gt;, and abbreviated variable names ## # ¹​main_name, ²​all_names, ³​birth_year, ⁴​death_year, ⁵​roles_titles, ## # ⁶​wikidata_item, ⁷​occupations This table contains further information about the nodes, each of which are identified by their unique ID. This can be joined to the network object using join() commands: sample_tbl_graph %&gt;% left_join(node_attributes, by = &#39;name&#39;) ## # A tbl_graph: 248 nodes and 364 edges ## # ## # A directed simple graph with 6 components ## # ## # Node Data: 248 × 13 (active) ## name main_n… all_na… links birth_… death_… gender roles_… wikida… occupa… ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 E000… Dr. La… Dr. La… http… 1632 1703 male chapla… http:/… chapla… ## 2 E000… Edward… Edward… http… 1638 1697 male astron… http:/… astron… ## 3 E000… Dr. Ri… Dr. Ri… http… 1619 1681 male priest http:/… priest ## 4 E000… Solms-… Prince… http… 1602 1675 female art co… http:/… art co… ## 5 E000… Arthur… Arthur… http… 1614 1686 male politi… http:/… politi… ## 6 E000… Sir Jo… Sir Jo… http… 1603 1671 male politi… http:/… politi… ## # … with 242 more rows, and 3 more variables: place_of_birth &lt;chr&gt;, ## # place_of_death &lt;chr&gt;, politician &lt;chr&gt; ## # ## # Edge Data: 364 × 3 ## from to weight ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 89 1 ## 2 2 22 4 ## 3 2 199 1 ## # … with 361 more rows Using this approach you can now make subsets of the network, and calculate global or node-level statistics for these. This example would return a network containing only individuals with politician listed as one of their occupations, for example: sample_tbl_graph %&gt;% left_join(node_attributes, by = &#39;name&#39;) %&gt;% # first join the attributes table again filter(str_detect(occupations, &quot;politician&quot;)) %&gt;% # returns any row with the string &#39;politician&#39; in it mutate(degree = centrality_degree(weights = weight, mode = &#39;all&#39;)) # calculate network metrics on this new subset of the data. ## # A tbl_graph: 94 nodes and 106 edges ## # ## # A directed simple graph with 18 components ## # ## # Node Data: 94 × 14 (active) ## name main_n… all_na… links birth_… death_… gender roles_… wikida… occupa… ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 E000… Arthur… Arthur… http… 1614 1686 male politi… http:/… politi… ## 2 E000… Sir Jo… Sir Jo… http… 1603 1671 male politi… http:/… politi… ## 3 E001… Henry … Henry … http… 1618 1685 male politi… http:/… politi… ## 4 E002… Willia… Willia… http… 1649 1717 male politi… http:/… politi… ## 5 E002… Edward… Edward… http… 1623 1683 male politi… http:/… politi… ## 6 E002… Roger … Roger … http… 1621 1679 male writer… http:/… writer… ## # … with 88 more rows, and 4 more variables: place_of_birth &lt;chr&gt;, ## # place_of_death &lt;chr&gt;, politician &lt;chr&gt;, degree &lt;dbl&gt; ## # ## # Edge Data: 106 × 3 ## from to weight ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 33 1 ## 2 2 33 1 ## 3 3 10 1 ## # … with 103 more rows Node attributes could be added and used to filter at several steps: before the network is created, and before or after network metrics are calculated. What differences might these make? Make sure you know at which stage you are calculating network metrics. If you calculate them after filtering, you’ll get a set of metrics based on a new, subsetted network. This new table can be outputted as a dataframe, as above. Here we use this to calculate the highest-degree nodes from the ‘polticians network’, keep their real names, and sort in descending order of degree: sample_tbl_graph %&gt;% left_join(node_attributes, by = &#39;name&#39;) %&gt;% # first join the attributes table again filter(str_detect(occupations, &quot;politician&quot;)) %&gt;% # returns any row containing the string &#39;politician&#39; mutate(degree = centrality_degree(weights = weight, mode = &#39;all&#39;)) %&gt;% # calculate network metrics on this new subset of the data. as_tibble() %&gt;% arrange(desc(degree)) %&gt;% select(name, main_name, degree) ## # A tibble: 94 × 3 ## name main_name degree ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 E022443-S042999-T000000 Williamson, Joseph (Sir) 174 ## 2 E001993-S018912-T000000 Henry Bennet, Earl of Arlington 54 ## 3 E004654-S006979-T000000 Charles II, King of England, Scotland, and Ir… 53 ## 4 E002620-S051125-T000000 William Blathwaite 47 ## 5 E002622-S014197-T000000 Edward Conway, Earl of Conway 30 ## 6 S016294 Francis, Lord Aungier 28 ## 7 E903376-S024822-T000000 Witt, Johan de 28 ## 8 S047442 Thomas Belasyse, Earl of Fauconberg 26 ## 9 E921955-S041743-T000000 Sir George Rawdon 25 ## 10 E021031-S049939-T000000 Van Beuningen 21 ## # … with 84 more rows Transitivity, Triads, structural balance Triad Census What this tells us about the macro-structure of a network Group detection Components Cliques Community Detection Conclusions "],["week-5-class-1-visualising-networks-with-ggraph.html", "Week 5, Class 1: Visualising Networks with ggraph Network visualisations Network Visualisations with R and ggraph. What makes a good network visualisation? Case study: Scientists and Politicians Conclusions", " Week 5, Class 1: Visualising Networks with ggraph Network visualisations Many analyses of networks rely on visualisation. Graphing a network is particularly useful for descriptive data analysis, as a way of describing the overall structure of graph, and exploratory data analysis, where it’s used as a sort of map to understand its various components, and to help spot patterns or interesting features by eye. These visualisations are most often the points and lines type diagrams which we have used throughout this book, but there are also a number of other ways they can be visualised (which often might be more useful). It’s important to note that there is nothing inherently spatial about a graph: it is simply a record of connections between nodes. When we choose to represent it visually, we have to make decisions as to its form and how precisely its nodes and edges are placed in 2D (or even 3D) space. From early social network research, researchers tried to manually visualise these graphs in meaningful ways, for example by placing closely-connected clusters together (and away from other clusters), placing important or highly-connected nodes towards the centre, or minimising the number of edge (line) crossings. With large networks, today this process is usually carried out using algorithms to work out the node placements. Force-directed network visualisations The most common family of these algorithms are ‘force-directed’, meaning they use a simulation of physical forces in order to create sensible placements of nodes. One of the most common of these is the Fruchterman-Reingold layout, which treats edges like a spring. Nodes which share an edge are attracted to each other using a spring-like force; every pair of nodes in the system also has a repulsive force. The algorithm simulates this physical system and stops when the distances between the nodes means that the system is in equilibrium. A good force-directed graph can actually convey a great deal of information about a network. The paper for this week’s reading argues that the ambiguity of a force-directed graph can actually make them very useful for exploratory data analysis, when they are interpreted correctly. Reading a network graph Using a similar approach to that paper, consider this network of book publishers from the eighteenth century: This is a large network, consisting of tens of thousands of nodes and millions of edges. The nodes are coloured by ‘community’, meaning tthat each colour of nodes is more densely connected to each other than to the nodes of other colours. Despite its size, some structure can be seen. We could describe it as the following: Spatially (ignoring the colours), there are three main sections: a large central section, and two smaller sections, one to the bottom-right and another, even smaller, to the top-left of the main section. This main section is shaped a bit like a hairbrush: it has an elongated ‘handle’, and an attached ‘brush’ at the top. The colours are distinct, meaning that the force-directed graph did a good job in replicating the clusters found by the community detection algorithm. To understand why it might have this shape, we looked into the nodes in each of its clusters. The first thing we noted was that the almost separate ‘islands’ were groups of Dublin (larger and closer island) and US publishers. The main section is made up of clusters of different time periods. Each time period is connected to the others mainly by a short edge, meaning that (for example) the 1700 - 1720 cluster is much more connected to the 1720 - 1740 cluster than to the 1780 - 1800 cluster, for example. This is typical in a long, multi-generational dataset like this. The handle is mostly London-based publishers, and the ‘brush’ is a group of Scottish publishers. These Scottish publishers are mostly connected to the later London clusters (red/pink). Scottish publishers are much closer to the London core than either Dublin or US. Some inferences we might make from this diagram. US and Dublin publishers were very separate from a London/Edinburgh publishing axis. For London publishers, the strongest pull is temporal rather than any other aspect. Over time, the closeness of the Edinburgh and London publisher networks grew. When used correctly and with more knowledge about a network, force-directed diagrams like this can help to spot distinct clusters, structural ‘holes’, and other features of a network. Avoiding the dreaded ‘hairball’ Visualising large networks using these methods can often result in a large tangled mass of nodes and edges, known perojatively as a ‘hairball’. This is particularly true of large graphs without much of a tendency to cluster together, such as this graph of Facebook page networks(Rozemberczki, Allen, and Sarkar 2021). These graphs have limited use, even as exploratory data. There are some ways to mitigate against them, however: Consider filtering the network, as in the previous steps. Make sure you’re aware of the consequences of filtering before and after you calculate network metrics, however. For large graphs, software such as Gephi, because it gives a real-time feedback of a network visualisation, can be useful, rather than purely using a programming language such as R. Think about whether a network diagram (or a network model at all) is the best way to represent or display your data. Could you arrive at the same conclusions with a simpler data analysis and output, such as a bar chart? Other Network Visualisations There are many other ways besides a ‘force-directed’ graph to visualise a network. Bipartite graph Some network types are particularly suited to other visualisation methods. Bipartite networks, for example, are often visualised so that the nodes are placed in two rows, according to their types. The positions within the rows are then determined by an algorithm designed to minimise edge crossings. EL = c(1,7,2,6,3,8,2,5,4,6,1,5,2,6) types = rep(0:1, each=4) g = make_bipartite_graph(types, EL, directed = FALSE) g %&gt;% ggraph(&#39;bipartite&#39;)+ geom_edge_link() + geom_node_point(size = 10, aes(color = as.character(types))) + coord_flip() + theme_void() + theme(legend.position = &#39;bottom&#39;) + labs(color = &#39;Type:&#39;) Adjacency Matrix One popular alternative to a network diagram is an adjacency matrix. In this case, the x and y axes contain each name in the network. A filled square is drawn for each edge, where they intersect. This method can be particularly useful for small, dense networks. Network Visualisations with R and ggraph. Visualisations like the examples above can be created with another R package, called ggraph. This uses the same basic syntax as the plotting library ggplot2, we used in an earlier lesson, but adds some special functions to visualise networks. To create a network diagram,we first create a network object as in previous lessons. library(tidygraph) library(tidyverse) library(ggraph) letters = read_csv(&quot;letter_data.csv&quot;, col_types = cols(.default = &quot;c&quot;)) edge_list = letters %&gt;% group_by(from_id, to_id) %&gt;% tally(name = &#39;weight&#39;) sample_tbl_graph = edge_list %&gt;% as_tbl_graph() This object is then passed to the function ggraph(), using the pipes. ggraph() tells R to begin drawing a graph. It has optional arguments: for example, you can set the layout to something other than the default using the argument layout =. sample_tbl_graph %&gt;% ggraph(layout = &#39;fr&#39;) You’ll notice that it doesn’t draw anything other than a blank grey background. ggraph uses the syntax as the plotting library ggplot2. Recall from Week 1, class 2, that plots in ggplot2 are created by adding geom_ functions to the blank plot. The same principle applies here, except with special ggraph geoms: First, geom_node_point(), will draw the nodes of your network as points. The nodes are positioned using the chosen (or default) layout algorithm. Next, geom_edge_link() will drawn the edges as connecting lines. sample_tbl_graph %&gt;% ggraph(&#39;fr&#39;) + geom_node_point() + geom_edge_link() There are many additional ways you can manipulate the visual appearance of your nodes and edges, for example by adding color, size, shape, arrows, and so forth. In ggraph (and ggplot), visual elements can be manipulated in two ways: you can either specify a value for a visual property, or you can specify that a property is mapped to a particular data point. This is all done with the relevant geom. To do the former, specify the value directly in the geom, like this: sample_tbl_graph %&gt;% ggraph(&#39;fr&#39;) + geom_node_point(size = 3) + # we specified that size should be set at the value 3 geom_edge_link() To map data to a particular data point, specify the data to be used within the geom, within a command aes(): sample_tbl_graph %&gt;% mutate(degree = centrality_degree()) %&gt;% # calculate a value for degree ggraph(&#39;fr&#39;) + geom_edge_link() + # switch around the order of the nodes and edges as it&#39;s easier to see the node color geom_node_point(size = 3, aes(color = degree)) # as well as the size, color is now mapped to the degree score. Other visual properties which can be mapped or specified include shape and alpha. The visual appearance of edges can also be adjusted, using the same syntax. Change to a dashed line using linetype: sample_tbl_graph %&gt;% mutate(degree = centrality_degree()) %&gt;% ggraph(&#39;fr&#39;) + geom_edge_link(linetype = 5) + geom_node_point(size = 3, aes(color = degree)) You can also add arrows, with the following syntax. The length and endcap arguments control the appearance of the arrow. sample_tbl_graph %&gt;% mutate(degree = centrality_degree()) %&gt;% ggraph(&#39;fr&#39;) + geom_edge_link(linetype = 5, arrow = arrow(length = unit(4, &#39;mm&#39;)), end_cap = circle(3, &#39;mm&#39;)) + geom_node_point(size = 3, aes(color = degree)) Ggraph and other tidyverse functions You can use the dplyr pipes to perform calculations, filter the data and then visualise it, all in one go: sample_tbl_graph %&gt;% activate(nodes)%&gt;% mutate(degree = centrality_degree(mode = &#39;total&#39;)) %&gt;% # calculate total degree filter(degree &gt;2) %&gt;% # filter to only include nodes with a degree greater than 2. ggraph(&#39;fr&#39;) + geom_node_point() + geom_edge_link() Add node labels Add geom_node_text() to add text labels to your network. In a larger network, it can be helpful to only show labels belonging to the most-connected nodes. To do this, join the people table to the nodes table, then use ggraph, setting the label aesthetic. Another dplyr verb, if_else allows you to add conditions to the label command. Here, I’ve used if_else to return the label if the node’s degree score is more than 10: A the moment, we only have the IDs for each node. In a further step we can join back the actual names and use these as labels. sample_tbl_graph %&gt;% activate(nodes) %&gt;% mutate(degree = centrality_degree(mode = &#39;total&#39;)) %&gt;% ggraph(&#39;nicely&#39;) + geom_node_point(aes(size = degree)) + geom_node_text(aes(label = if_else(degree &gt;10, name, NULL), size = degree), repel = TRUE) + geom_edge_link(alpha = .2) ## Warning: Removed 240 rows containing missing values (geom_text_repel). Calculating and coloring by community detection One very common visualisation is colouring the various groups in the network, which might give us a clue as to its structure. sample_tbl_graph %&gt;% as.undirected() %&gt;% as_tbl_graph() %&gt;% activate(nodes) %&gt;% mutate(degree = centrality_degree(mode = &#39;total&#39;)) %&gt;% mutate(community = group_edge_betweenness(weights =NULL)) %&gt;% filter(community %in% 1:4) %&gt;% ggraph(&#39;nicely&#39;) + geom_node_point(aes(size = degree, color = as.character(community))) + geom_edge_link(alpha = .2) ## Warning in cluster_edge_betweenness(graph = .G(), weights = weights, directed = ## directed): At core/community/edge_betweenness.c:485 : Membership vector will be ## selected based on the lowest modularity score. ## Warning in cluster_edge_betweenness(graph = .G(), weights = weights, directed ## = directed): At core/community/edge_betweenness.c:492 : Modularity calculation ## with weighted edge betweenness community detection might not make sense -- ## modularity treats edge weights as similarities while edge betwenness treats them ## as distances. What makes a good network visualisation? You will rarely be able to make a good network visualisation simply by pushing a button and using the default settings. You should think about what your network is trying to communicate and to what audience, and spend some time Treat the diagram as carefully as you would an essay or other piece of text. Is it easy to understand? Make sure you really need a network graph, and that another method of communicating the information wouldn’t be better. For example, if your diagram is simply showing the most influential nodes, would a bar chart (or even a table) with their degree work just as well? Use colour/shape/size if you need to, but make sure you explain what each element is doing. Spend some time showing your readers how to ‘read’ the visualisation. What is interesting about the left/right/top/bottom of the diagram? What is the significance of nodes grouped together, and what about nodes in the centre versus nodes towards the edge? Are there obvious clusters and gaps, and do these show up in the visualisation? Case study: Scientists and Politicians In this final section, I’ll demonstrate how these methods might be used to carry out exploratory data analysis on the sample letter dataset. The node attributes can used either to filter the network, or added as extra visual elements. Doing so helps us to understand a bit more about why the network might look the way it does. First, load the same node attributes table as before: node_attributes = read_csv(&#39;node_attributes.csv&#39;) ## Rows: 248 Columns: 13 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (11): name, main_name, all_names, links, gender, roles_titles, wikidata_... ## dbl (2): birth_year, death_year ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. This external information might help us to make sense of the clusters found by the community detection algorithm used above. One of the fields in the data is whether that person is listed as a politician on Wikidata: the field is a simple flag of either yes or no. To check whether this might be a clue towards the structure of the network, we will set the color to the community detection results, and the shape to the politician flag, with the following code: sample_tbl_graph %&gt;% as.undirected() %&gt;% as_tbl_graph()%&gt;% activate(nodes) %&gt;% mutate(degree = centrality_degree(mode = &#39;total&#39;)) %&gt;% left_join(node_attributes) %&gt;% mutate(community = group_edge_betweenness(weights =NULL)) %&gt;% filter(community %in% 1:4) %&gt;% ggraph(&#39;fr&#39;) + geom_edge_link(alpha = .2)+ geom_node_point(aes(size = degree, color = as.character(community), shape = politician)) + geom_node_text(aes(label = ifelse(degree &gt;2, main_name, NA)), size = 2.5, repel = T) + theme(legend.position = &#39;none&#39;) It does look like one of the four clusters (cluster 2) has far fewer politicians. This cluster seems to be a group of what might be described as natural philosophers. Other things to look out for in this visualisation: What is the importance of the nodes sitting in between the scientists and the politicians? What metrics might they score highly on, and what role might they occupy in this system? What kinds of information may they be able to pass on? What is the position of Athanasius Kircher? What does it mean to be on the periphery of this network? What effect might additional data have on the metrics in this network? Should this make us cautious about any inference from this network? Conclusions Hopefully, from reading this chapter, you’ll be convinced that network visualisations are useful, but that they should also be approached with caution. The bottom line is, visualisations of themselves are not a result: at the very least, they need extensive commentary in order to explain them, and in some cases, they may simply be useful ways of describing the network. At the same time, in combination with the additional attributes from our data model, thoughtful visualisations can be incredibly useful for exploring a network dataset, revealing patterns that are otherwise hidden. I encourage you to use visualisations in your final project if they make sense, are not just ‘hairballs’, and if they can be justified with relevant commentary. References "],["week-5-class-2-other-network-types.html", "Week 5, class 2: Other network types Ego Networks Case Study: A Spotify Ego Network Bipartite networks Co-occurrence and co-authorship networks. Conclusions Exercises:", " Week 5, class 2: Other network types Ego Networks Very often when applying network analysis to humanities datasets, you’ll be working with ego networks. An ego network is a network seen from the perspective of a focal node, known as the ego, and all the nodes to which it is connected, known as alters. In a true ego network, the network will include all the links between the alters. Figure 6: Example ego network These types of networks are very common in real-world networks because data is often collected from the perspective of an ego node or a number of ego nodes, for example, in a survey, or perhaps in a collection of correspondence. Because of this it is important to understand what they are, and think about whether the data is representative. At the same time, ego network data can be very useful. Studies have shown that many metrics in an ego network correlate strongly with metrics from a full network (meaning that if an alter is central to the ego network, it is likely also central if we had the full network data too. Network analysis from the point of view of egos can tell us about the structure of a particular node and its relationship to its neighbours. Comparing many ego networks can be used to good effect. Lerman et al (2022) created a list of features (attributes) for the citation ego networks of authors elected to the National Academy of Sciences, successfully using those features to predict the gender of the author: Women’s ego networks have higher average degree, edge density, and clustering coefficient. Together, these features suggest that women are more tightly embedded within their research communities. This is consistent with previous findings that women tend to gravitate to certain communities (11). Women have fewer peers than men, but these peers are more productive (publish more papers) and receive more citations. Finally, women NAS members have more women among their peers. Many of the datasets you might be considering using for your final project may in fact be ego networks. It is worth thinking about the implications of this, and considering which kinds of metrics, or comparisons would work best. Is each ego network collected using the same method? If not, does this skew the results? Case Study: A Spotify Ego Network This section walks through the code for creating and analysing an ego network drawn from a very different type of humanities dataset: Spotify’s ‘related’ artists information. This is available through Spotify’s API. One function of the API lets us download the twenty ‘most-related’ artists to a given ‘seed’ artist. The algorithm for how the most-related are calculated is not public, but it’s likely by comparing overlapping listeners (if lots of people listen to both artist X and artist Y, those artists are marked as related). Requirements If you want to complete the (optional) exercise in the corresponding file on CSC notebooks, you’ll need to sign up for a Spotify developer account. Once you’ve done this, create a new application using the dashboard. This will give you the necessary client ID and password. You’ll then need to install the spotifyr package using install.packages('spotifyr). An exercise relating to the bipartite network below is another option - if for whatever reason you don’t want to sign up for an account, or have difficulty accessing the API. The data for this is already available. Get related artists from the Spotify API. The spotifyr package makes it easy to access the Spotify API functions using R. Once you have installed the spotifyr package, you need to tell it where to find your user ID and password. Copy these from your dashboard on the Spotify API page. Then, use the following code to temporarily store them in R’s memory: #Sys.setenv(SPOTIFY_CLIENT_ID = &#39;&#39;) # uncomment these lines and add your client ID and password here, in between the quotation marks. #Sys.setenv(SPOTIFY_CLIENT_SECRET = &#39;&#39;) Next, you’ll use the command get_spotify_access_token() to use this ID and password to generate an access token. Following this, we need to find the spotify ID for the ‘ego’ artist. The easiest way to do this is with the get_artist_audio_features() function. This can take a character string and will return the closest match, if an ID is not specified. To get the ID for the band Radiohead, we do the following: # install.packages(&#39;spotifyr&#39;) # install the package if necessary library(spotifyr) access_token &lt;- get_spotify_access_token() radiohead &lt;- get_artist_audio_features(&#39;radiohead&#39;) glimpse(radiohead) ## Rows: 193 ## Columns: 39 ## $ artist_name &lt;chr&gt; &quot;Radiohead&quot;, &quot;Radiohead&quot;, &quot;Radiohead&quot;, &quot;R… ## $ artist_id &lt;chr&gt; &quot;4Z8W4fKeB5YxbusRsdQVPb&quot;, &quot;4Z8W4fKeB5Yxbu… ## $ album_id &lt;chr&gt; &quot;6ofEQubaL265rIW6WnCU8y&quot;, &quot;6ofEQubaL265rI… ## $ album_type &lt;chr&gt; &quot;album&quot;, &quot;album&quot;, &quot;album&quot;, &quot;album&quot;, &quot;albu… ## $ album_images &lt;list&gt; [&lt;data.frame[3 x 3]&gt;], [&lt;data.frame[3 x … ## $ album_release_date &lt;chr&gt; &quot;2021-11-05&quot;, &quot;2021-11-05&quot;, &quot;2021-11-05&quot;,… ## $ album_release_year &lt;dbl&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2021,… ## $ album_release_date_precision &lt;chr&gt; &quot;day&quot;, &quot;day&quot;, &quot;day&quot;, &quot;day&quot;, &quot;day&quot;, &quot;day&quot;,… ## $ danceability &lt;dbl&gt; 0.296, 0.630, 0.488, 0.167, 0.165, 0.402,… ## $ energy &lt;dbl&gt; 0.463, 0.428, 0.754, 0.302, 0.146, 0.757,… ## $ key &lt;int&gt; 5, 5, 2, 6, 6, 7, 0, 3, 2, 7, 7, 0, 11, 1… ## $ loudness &lt;dbl&gt; -11.412, -15.520, -8.552, -11.644, -21.35… ## $ mode &lt;int&gt; 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,… ## $ speechiness &lt;dbl&gt; 0.0449, 0.0358, 0.0378, 0.0345, 0.0362, 0… ## $ acousticness &lt;dbl&gt; 7.05e-01, 2.62e-01, 2.37e-03, 3.16e-01, 8… ## $ instrumentalness &lt;dbl&gt; 4.82e-02, 8.52e-01, 8.51e-01, 7.97e-01, 8… ## $ liveness &lt;dbl&gt; 0.0954, 0.2780, 0.2240, 0.1100, 0.1090, 0… ## $ valence &lt;dbl&gt; 0.0629, 0.1590, 0.3880, 0.1900, 0.0577, 0… ## $ tempo &lt;dbl&gt; 123.943, 112.923, 91.517, 102.026, 134.50… ## $ track_id &lt;chr&gt; &quot;62dUmjtkYOUwYOALt6pefh&quot;, &quot;797AyTQcqoQgqW… ## $ analysis_url &lt;chr&gt; &quot;https://api.spotify.com/v1/audio-analysi… ## $ time_signature &lt;int&gt; 5, 4, 4, 4, 3, 4, 4, 3, 5, 4, 3, 4, 4, 4,… ## $ artists &lt;list&gt; [&lt;data.frame[1 x 6]&gt;], [&lt;data.frame[1 x … ## $ available_markets &lt;list&gt; &lt;&quot;AD&quot;, &quot;AE&quot;, &quot;AG&quot;, &quot;AL&quot;, &quot;AM&quot;, &quot;AO&quot;, &quot;AR… ## $ disc_number &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2,… ## $ duration_ms &lt;int&gt; 251426, 284506, 351693, 356333, 222600, 3… ## $ explicit &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,… ## $ track_href &lt;chr&gt; &quot;https://api.spotify.com/v1/tracks/62dUmj… ## $ is_local &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,… ## $ track_name &lt;chr&gt; &quot;Everything In Its Right Place&quot;, &quot;Kid A&quot;,… ## $ track_preview_url &lt;chr&gt; &quot;https://p.scdn.co/mp3-preview/74e765ad68… ## $ track_number &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1, 2, … ## $ type &lt;chr&gt; &quot;track&quot;, &quot;track&quot;, &quot;track&quot;, &quot;track&quot;, &quot;trac… ## $ track_uri &lt;chr&gt; &quot;spotify:track:62dUmjtkYOUwYOALt6pefh&quot;, &quot;… ## $ external_urls.spotify &lt;chr&gt; &quot;https://open.spotify.com/track/62dUmjtkY… ## $ album_name &lt;chr&gt; &quot;KID A MNESIA&quot;, &quot;KID A MNESIA&quot;, &quot;KID A MN… ## $ key_name &lt;chr&gt; &quot;F&quot;, &quot;F&quot;, &quot;D&quot;, &quot;F#&quot;, &quot;F#&quot;, &quot;G&quot;, &quot;C&quot;, &quot;D#&quot;… ## $ mode_name &lt;chr&gt; &quot;minor&quot;, &quot;major&quot;, &quot;major&quot;, &quot;minor&quot;, &quot;majo… ## $ key_mode &lt;chr&gt; &quot;F minor&quot;, &quot;F major&quot;, &quot;D major&quot;, &quot;F# mino… This returns all of Radiohead’s tracks along with some more information. The second column in each row is the artist_id, in this case 4Z8W4fKeB5YxbusRsdQVPb, which we will use as the input for further functions. The function to get the related artists to one artist is get_related_artists(). The below syntax will get Radiohead’s related artists list: radiohead_related = spotifyr::get_related_artists(&#39;4Z8W4fKeB5YxbusRsdQVPb&#39;) glimpse(radiohead_related) ## Rows: 20 ## Columns: 11 ## $ genres &lt;list&gt; &lt;&quot;art pop&quot;, &quot;electronica&quot;, &quot;glitch pop&quot;, &quot;indie… ## $ href &lt;chr&gt; &quot;https://api.spotify.com/v1/artists/4CvTDPKA6W06… ## $ id &lt;chr&gt; &quot;4CvTDPKA6W06DRfBnZKrau&quot;, &quot;7MhMgCo0Bl0Kukl93PZbY… ## $ images &lt;list&gt; [&lt;data.frame[3 x 3]&gt;], [&lt;data.frame[3 x 3]&gt;], [… ## $ name &lt;chr&gt; &quot;Thom Yorke&quot;, &quot;Blur&quot;, &quot;Pixies&quot;, &quot;Portishead&quot;, &quot;J… ## $ popularity &lt;int&gt; 58, 68, 72, 61, 61, 60, 58, 59, 59, 56, 64, 64, … ## $ type &lt;chr&gt; &quot;artist&quot;, &quot;artist&quot;, &quot;artist&quot;, &quot;artist&quot;, &quot;artist&quot;… ## $ uri &lt;chr&gt; &quot;spotify:artist:4CvTDPKA6W06DRfBnZKrau&quot;, &quot;spotif… ## $ external_urls.spotify &lt;chr&gt; &quot;https://open.spotify.com/artist/4CvTDPKA6W06DRf… ## $ followers.href &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ followers.total &lt;int&gt; 862475, 2625721, 2309862, 1643011, 899033, 76737… The result is a dataframe with a list of the 20 closest-related artists to Radiohead, using Spotify’s algorithm, with some more information including the artists’ popularity and genres - and their Spotify ID. This can be visualised using the method from the previous lesson: library(ggraph) library(tidygraph) library(igraph) radiohead_related %&gt;% mutate(ego_name = &#39;Radiohead&#39;) %&gt;% distinct( ego_name, name) %&gt;% as_tbl_graph() %&gt;% ggraph(&#39;stress&#39;) + geom_edge_link() + geom_node_point(size = 20, fill = &#39;white&#39;, color = &#39;black&#39;, pch = 21)+ geom_node_text(size = 3, aes(label =name)) ‘Crawling’ through the related artists information As you might be able to tell, there is very little useful information in this type of ego network: which only includes links to and from the ego node. Each ‘alter’ has a degree of one, and will have the same scores for all network metrics. However, we can use the new artists’ IDs to ‘crawl’ through the API and get the 20 most related artists for each of them, taking a snowball sample approach to creating a network. The easiest way to do this is to use a R function lapply, which applies a function to every element in a list, and returns a new list. radiohead_related_related = lapply(radiohead_related$id, get_related_artists) radiohead_related_related = data.table::rbindlist(radiohead_related_related) radiohead_related_related = radiohead_related_related %&gt;% mutate(seed_id = rep(radiohead_related$id, each= 20))%&gt;% mutate(ego_name = rep(radiohead_related$name, each= 20)) This results in dataset of the twenty related nodes plus all their own connections, which can be drawn as a network: g = radiohead_related_related %&gt;% rbind(radiohead_related %&gt;% mutate(seed_id = &#39;4Z8W4fKeB5YxbusRsdQVPb&#39;)%&gt;% mutate(ego_name = &#39;Radiohead&#39;) ) %&gt;% distinct(ego_name, name) %&gt;% as_tbl_graph(directed = F) %&gt;% ggraph(&#39;fr&#39;)+ geom_edge_link(alpha = .5) + geom_node_point(size = 4, color = &#39;white&#39;) + geom_node_text(size = 2, aes(label =name)) g This network links all the alters to all 20 of their closest-related artists. You can see that many of the artists are not actually connected to: it is all the alters plus all their additional related artists. To simplify things, we can start with a ‘true’ ego network, which just includes the ego (Radiohead), the alters (the 20 related artists) and the alter links (any pairs of alters who are also related to each other). radiohead_related_related %&gt;% rbind(radiohead_related %&gt;% mutate(seed_id = &#39;4Z8W4fKeB5YxbusRsdQVPb&#39;) %&gt;% mutate(ego_name = &#39;Radiohead&#39;) ) %&gt;% filter(id %in% radiohead_related$id) %&gt;% distinct(ego_name, name) %&gt;% as_tbl_graph(directed = F) %&gt;% mutate(group = group_louvain()) %&gt;% ggraph(&#39;fr&#39;)+ geom_edge_link(alpha = .5) + geom_node_point(size = 10, aes(color = as.character(group))) + geom_node_text(size = 3, aes(label =name)) We can run community detection on the network to look for distinct clusters. Adding these alters we can see some clusters already emerging: 90s britpop (The Verve, Blur, Pulp), triphop (Portishead), Shoegaze (Slowdive, Mazzy Star), College rock/indie folk (NMH, Elliot Smith): This ego network can tell us a number of things: There are a large number of triads, meaning that artists are clustered together (if the Verve and Pulp are connected to Blur, it’s also likely that they are connected to each other). What does a clustered network mean in this circumstance? There is almost a ‘clique’ (a group of fully-connected nodes), including all those in green. What does a clique tell us? There is one node (Smashing Pumpkins) with no other connections. As a further step, we can crawl another level deep into the related artists’ network. This will give us all the connections at a third level removed from Radiohead. level_3_related = lapply(radiohead_related_related$id, get_related_artists) level_3_related = level_3_related %&gt;% data.table::rbindlist() %&gt;% mutate(seed_id = rep(radiohead_related_related$id, each= 20))%&gt;% mutate(ego_name = rep(radiohead_related_related$name, each= 20)) glimpse(level_3_related) ## Rows: 8,000 ## Columns: 13 ## $ genres &lt;list&gt; &lt;&quot;art pop&quot;, &quot;electronica&quot;, &quot;glitch pop&quot;, &quot;indie… ## $ href &lt;chr&gt; &quot;https://api.spotify.com/v1/artists/4CvTDPKA6W06… ## $ id &lt;chr&gt; &quot;4CvTDPKA6W06DRfBnZKrau&quot;, &quot;2f88S1uYsEwP0n4x36wvG… ## $ images &lt;list&gt; [&lt;data.frame[3 x 3]&gt;], [&lt;data.frame[3 x 3]&gt;], [… ## $ name &lt;chr&gt; &quot;Thom Yorke&quot;, &quot;Ultraísta&quot;, &quot;Modeselektor&quot;, &quot;Jonn… ## $ popularity &lt;int&gt; 58, 32, 48, 47, 39, 40, 45, 38, 50, 57, 29, 50, … ## $ type &lt;chr&gt; &quot;artist&quot;, &quot;artist&quot;, &quot;artist&quot;, &quot;artist&quot;, &quot;artist&quot;… ## $ uri &lt;chr&gt; &quot;spotify:artist:4CvTDPKA6W06DRfBnZKrau&quot;, &quot;spotif… ## $ external_urls.spotify &lt;chr&gt; &quot;https://open.spotify.com/artist/4CvTDPKA6W06DRf… ## $ followers.href &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ followers.total &lt;int&gt; 862475, 23202, 217589, 139224, 22720, 111498, 97… ## $ seed_id &lt;chr&gt; &quot;7tA9Eeeb68kkiG9Nrvuzmi&quot;, &quot;7tA9Eeeb68kkiG9Nrvuzm… ## $ ego_name &lt;chr&gt; &quot;Atoms For Peace&quot;, &quot;Atoms For Peace&quot;, &quot;Atoms For… df = level_3_related %&gt;% #filter(id %in% radiohead_related_related$id)%&gt;% distinct(id, seed_id) %&gt;% as_tbl_graph(directed = F) %&gt;% mutate(total_degree = centrality_degree(mode = &#39;all&#39;)) %&gt;% filter(total_degree&gt;1) %&gt;% mutate(group = group_louvain()) df %&gt;% ggraph(&#39;fr&#39;) + geom_edge_link(alpha = .1) + geom_node_point(aes(color =as.character(group))) In this extended network, clusters are clearly visible again. There are too many nodes now to label them or easily divide them into genres. Instead, we’ll use summarise functions to make a table of information on each of the groups. genres = level_3_related %&gt;% distinct(id, genres, name) genres_total = df %&gt;% as_tibble()%&gt;% inner_join(genres, by = c(&#39;name&#39; = &#39;id&#39;)) %&gt;% unnest(genres) %&gt;% count(group,genres) %&gt;% arrange(desc(n)) %&gt;% mutate(genres = paste0(genres, &quot; (&quot;, n, &quot;)&quot;)) %&gt;% group_by(group) %&gt;% slice_max(order_by = n, n = 20) %&gt;% summarise(genres_total = paste0(genres, collapse = &#39; ;&#39;)) artist_total = df %&gt;% as_tibble() %&gt;% inner_join(genres, by = c(&#39;name&#39; = &#39;id&#39;)) %&gt;% mutate(name.y = paste0(name.y, &quot; (&quot;, total_degree, &quot;)&quot;)) %&gt;% arrange(desc(total_degree)) %&gt;% group_by(group) %&gt;% slice_max(order_by = total_degree, n = 20) %&gt;% summarise(artists = paste0(name.y, collapse = &#39; ;&#39;)) genres_total %&gt;% left_join(artist_total) %&gt;% DT::datatable() ## Joining, by = &quot;group&quot; Most importantly, this shows that it’s not always necessary to have access to the ‘full’ network in order to get results. In many cases, we will work with ego networks or sets of connected ego networks. Even so, it is possible to make interesting insights into the data regardless. The key takeaway is that you should take into account the perspective you’re looking from. Network metrics Using the techniques from the previous lesson, we can make a table of network metrics: node_info = level_3_related %&gt;% distinct(id, name, genres) %&gt;% rbind(level_3_related %&gt;% select(id = seed_id,name = ego_name, genres) %&gt;% distinct(id, name, genres)) %&gt;% distinct(id, name, genres) %&gt;% select(name = id, artist_name = name, genres) %&gt;% distinct(name, .keep_all = T) g = level_3_related %&gt;% #filter(id %in% radiohead_related_related$id) %&gt;% distinct(id, seed_id) %&gt;% as_tbl_graph(directed = TRUE) %&gt;% left_join(node_info, by = &#39;name&#39;) %&gt;% mutate(in_degree = centrality_degree(mode = &#39;in&#39;))%&gt;% mutate(out_degree = centrality_degree(mode = &#39;out&#39;))%&gt;% mutate(total_degree = centrality_degree(mode = &#39;all&#39;)) %&gt;% mutate(between = centrality_betweenness()) %&gt;% arrange(desc(total_degree)) ## Warning in betweenness(graph = graph, v = V(graph), directed = directed, : ## &#39;nobigint&#39; is deprecated since igraph 1.3 and will be removed in igraph 1.4 g %&gt;% as_tibble() ## # A tibble: 1,275 × 7 ## name artist_name genres in_de…¹ out_d…² total…³ between ## &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0sHeX8oQ6o7xic3wMf4NBU Supergrass &lt;chr&gt; 20 32 52 20033. ## 2 3mbVe260Kgvs1P8YFcCyY7 The Seahorses &lt;chr&gt; 20 30 50 646. ## 3 5vIOGcdmx1eIkq3ZtuS12U Ocean Colour S… &lt;chr&gt; 20 29 49 586. ## 4 5fScAXreYFnuqwOgBsJgSd The Charlatans &lt;chr&gt; 20 29 49 760. ## 5 3s398TKZNahAURRacx7oIT Ian Brown &lt;chr&gt; 20 29 49 1290. ## 6 2wrhBKGC3DTNNNDRJPaxW6 Sebadoh &lt;chr&gt; 20 29 49 8693. ## 7 0vBDEQ1aLZpe4zgn2fPH6Z Cast &lt;chr&gt; 20 28 48 907. ## 8 4oV5EVJ0XFWsJKoOvdRPvl Guided By Voic… &lt;chr&gt; 20 28 48 6691. ## 9 0LVrQUinPUBFvVD5pLqmWY Doves &lt;chr&gt; 20 26 46 22114. ## 10 6bGcpvHbRHg8s0wiNyIZK1 Kula Shaker &lt;chr&gt; 20 25 45 396. ## # … with 1,265 more rows, and abbreviated variable names ¹​in_degree, ## # ²​out_degree, ³​total_degree A few things to note: The network is asymmetric: if artist A is in the top twenty of artist B, it doesn’t necessarily follow that B is in the top twenty of A. We can test that by calculating the network’s reciprocity: g %&gt;% filter(name %in% radiohead_related_related$id) %&gt;% igraph::reciprocity() ## [1] 0.6144431 Just over 60% of links are reciprocated. It’s also transitive: g %&gt;% filter(name %in% radiohead_related_related$id) %&gt;% igraph::transitivity(&#39;global&#39;) ## [1] 0.5589212 55% of possible triangles are complete. The degree distribution is very particular to the sampling method. We have the ‘full graph’ for each of the 400 seed names (Radiohead’s top twenty plus all their top twenties), but for the rest of the network (the third level removed from Radiohead), we only have their links to the 400 seed names. These nodes all have an incoming degree of 0, because we don’t have any of their related artists, only the artists they are related to… g %&gt;% igraph::degree() %&gt;% hist(main = &quot;Histogram of degree distribution&quot;) What does this tell us about the way music is organised? Relatedness is transitive, meaning artists tend to form triangles: if both your friends like a band, it’s likely you’ll like them too. Spotify can exploit this fact to make better playlist recommendations! Relatedness is not necessarily reciprocal: there’s a difference between A related to B and B related to A. Why might this be? Bipartite networks The work we have done so far has been on networks which are naturally what is known as unimodel or unipartite: person A sends a letter to person B. In a letter network there is only one type of node (a letter author or recipient), and one type of node (sends/receives a letter). Many networks are not this straightforward, and have two, or more, types of nodes. For example, a network of twitter users connected to twitter groups, or a network of directors connected to companies. These networks are known as bimodal or bipartite, if there are two types of nodes, or tripartite when there’s three, and so forth. The following is a diagram of a bipartite network of Facebook users and group membership. In this network, the first type (red, on the left) are people, and the second type (green, on the right) are Facebook groups. A line is drawn from one to the other if they are a member of that group. Carol is a member of the rabbits and dog group, Carol, David, Bob, and Egbert are members of the dog group, and Egbert and Alice members of the cat group. In digital humanities research, we often have access to bipartite network data, because almost any two sets of data points can be represented as a bipartite network. In some cases, the dataset can be derived rather than some pre-existing membership or category the data belongs to, as, for instance, in this study of the New Zealand parliament, which constructed a bipartite network of MPs to a set of speech topics, created using LDA topic modelling. In this model MPs are the first type, and topics the second, and these are used to construct a network of MPs based on their similarity across the topics they spoke about in Parliament. From Curran B, Higham K, Ortiz E, Vasques Filho D (2018) Look who’s talking: Two-mode networks as representations of a topic model of New Zealand parliamentary speeches. PLoS ONE 13(6): e0199072. https://doi.org/10.1371/journal.pone.0199072 It’s important, therefore, to understand the extent to which regular network methods work or don’t work with this structure. Standard network measurements (such as degree) are easy to calculate using these networks, but are not always meaningful. In the above example, the degree count for each node (its connections) is simply a count of its group membership. Unlike in a regular network, the measurement doesn’t give any clues as to the most central member of the group. Similar problems exist for other metrics. In many cases, then, we will need to do something to the network in order to get meaningful analysis from it. The most common thing to do is to project the network. This involves collapsing the network, and directly connecting one of the node types, based on their connections to the other. For example, the network above can be collapsed into two separate networks: a network of people connected by shared group membership, and a network of groups connected by shared members: The network on the left displays a very common aspect of bipartite network projections: cliques, a cluster of nodes where each is connected to all the others. In the network on the left, the edge becomes ‘shares a Facebook group with’, and on the right, ‘has shared members’. Which of these two networks do you think is more appropriate? To a certain extent, that depends on the question. The more obvious answer would be to build a network of people, but if we were more interested in the ‘ecosystem’ of Facebook groups and how they interact, then perhaps the second network type would be of more use. At this point, regular network metrics can be used. We might use degree, for example, to demonstrate that Alice is peripheral to this network. However, it’s important to be aware of what projecting the network does. Most importantly, there is a potential loss of information: in the new network, the edge only records that there is a shared group between two nodes, and the information on which groups specifically were shared is discarded. Some of this information can be kept through a weight value attached to each node. In the diagram on the left above, Egbert and Bob have a weight of two, because they share two groups (cats and dogs). This weight information can be incorporated into your network metrics. Technically almost any data can be modelled as a bipartite network. However, is it always appropriate? To give a slightly ridiculous example, imagine you had a dataset of fruit, and their corresponding colours. fruit color apple green banana yellow peach pink pineapple yellow grape green There is nothing stopping you from turning this into a bipartite network of fruit connected to colours, and even projecting this to a network of fruit directly connected by shared colours. It is very easy to technically turn this into a network. But is it meaningful? Perhaps not, unless there was a very clear reason for doing (biologists may be interested in this very question!). Co-occurrence and co-authorship networks. However, there are many cases where a bipartite network does actually make sense. Two very popular (and related) uses are co-authorship networks and co-citation networks. In the former, people are connected to the papers they wrote together, and in the latter, they are connected if they were cited in the same paper together. Unlike fruit and colours, this data has some inherently networked-looking properties. The connections are likely to be clustered into different topics or academic communities, and if A and B both authored separate papers with C, they probably have a higher chance of also authoring a paper together. Perhaps even more interestingly, this is a way of finding connections where we otherwise may have no data. We probably don’t have any information on whether a large group of academic writers were in contact with each other (although maybe some of it can be found through Twitter data). A co-authorship network allows us to infer these connections through another dataset. This is very often the case in humanities datasets, particularly historical, where we only have very limited information on who was in contact with whom, and then, only if their letters or some other record of their contact survived. However, we may have more information on the companies they worked for, the groups they were part of, or the publications they worked on. Modelling this data as a network may allow us to understand subject boundaries, highlight influential individuals, and look, for example, at questions of gender or racial bias in patterns of authorship and citation. This recent paper constructed a co-authorship network of digital humanities publications, and found that even through there were less women authors overall, they had important roles as bridges, linking otherwise disconnected areas. This network diagram from that paper shows the centrality of many of the green (female) authors: From Gao, J., Nyhan, J., Duke-Williams, O. and Mahony, S. (2022), “Gender influences in Digital Humanities co-authorship networks”, Journal of Documentation, Vol. 78 No. 7, pp. 327-350. https://doi.org/10.1108/JD-11-2021-0221 A type of co-authorship: publisher networks We can take a similar approach to the information found in historical books. While co-authorship itself in early modern publishing was rare, most books were produced by collaborations between sets of publishers and printers. These relationships are well suited to modelling as a network, and we could imagine they might display some of the network tendencies we’ve just discussed. The dataset The dataset we’ll be working with the a dataset of metadata from the English Short Title Catalogue (known as the ESTC). This data lists a unique ID for each publisher, printer, bookseller and author listed on the title pages of books printed between 1500 and 1800. This information comes from the imprint of the book: the list of authors, printers, publishers and so forth found on the title page. Figure 7: Bipartite network from book imprints As you can see, the title page lists a few pieces of information: the book is ‘printed for’ A. Millar (who is the publisher), and ‘sold by’ Thomas Cadell, the bookseller. These pieces of information, showing a connection between Millar and Cadell, can form the basis of a bipartite network. Method In this class we’ll take this raw data, filter it, and turn it into a bipartite network of publishers and printers connected to books. We’ll then project the network, and directly connect the publishers and printers, based on their shared co-occurrences on books. This network can then be visualised and analysed. First, load the data into R: load(file = &#39;../publisher_network/estc_actor_links&#39;) load(&#39;../publisher_network/estc_core&#39;) The data is organised like this: each row represents a book and a single actor linked to that book (meaning a publisher, printer, bookseller, or author). Each actor and book has a unique code. Further columns give information on the type of actor (some can have multiple, for example be the author and the publisher). The book IDs and the actor IDs can be used as an edge list, and processed exactly the same as the previous lesson on regular one-mode networks. To do this, first filter to the appropriate types of actors, and then use select() to choose the actor_id and estc_id columns: edge_list = estc_actor_links %&gt;% left_join(estc_core %&gt;% select(estc_id, publication_year, publication_place)) %&gt;% filter(publication_year %in% 1750:1760) %&gt;% filter(publication_place == &#39;Edinburgh&#39;) %&gt;% filter(actor_role_publisher == TRUE) %&gt;% select(estc_id, actor_id) ## Joining, by = &quot;estc_id&quot; Use the same functions as before to turn this into a network object: publisher_graph = edge_list%&gt;% as_tbl_graph() The next steps are specific to bipartite networks. In order for R to ‘know’ that the network is bipartite, each node needs to have an associated type. We use an igraph function for this, called bipartite_mapping(). This assigns a TRUE or FALSE value to each node, depending on whether they are found in the first or second column of the data. This is saved as an attribute of the nodes using the following code: V(publisher_graph)$type &lt;- bipartite_mapping(publisher_graph)$type At this point, we can already visualise the network, setting the shape of the node to the type: publisher_graph %&gt;% ggraph(&#39;stress&#39;) + geom_edge_link(alpha = .2) + geom_node_point(aes(shape = type),size =2) + theme_void() It already looks like a network structure, with a number of disconnected components (individuals who never collaborate, or only in a small group), with a central connected ‘component’, consisting of publishers who often collaborate on books together. However, we want to know more specifically about the structure of the publisher network. To do this, we project it. Igraph has another function for this, bipartite_projection(). Use this function on the network: proj = bipartite.projection(publisher_graph) The result is a new object, a list, containing to further objects. These are the two network projections (publisher to publisher, and book to book). They can be accessed using proj[[1]] and proj[[2]]. We’ll work with the second item, the publishers, but first, turn it into a tidygraph object so that we can work on it in the same way as the previous lessons. proj[[2]] = proj[[2]] %&gt;% as_tbl_graph() proj[[2]] ## # A tbl_graph: 133 nodes and 308 edges ## # ## # An undirected simple graph with 66 components ## # ## # Node Data: 133 × 1 (active) ## name ## &lt;chr&gt; ## 1 46725009 ## 2 robertfleming_0 ## 3 andrewstevensonwriter_0 ## 4 willgordon_0 ## 5 alexanderkincaid_1 ## 6 johngray_0 ## # … with 127 more rows ## # ## # Edge Data: 308 × 3 ## from to weight ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2 4 2 ## 2 2 7 1 ## 3 2 11 1 ## # … with 305 more rows Now we have a new network, consisting of publisher nodes, and edges. Note, also, that the edges have a weight attached, representing the number of shared books they occur on. The rest of the steps can be copied directly from the previous lesson on networks. proj[[2]] %&gt;% mutate(degree = centrality_degree(mode = &#39;all&#39;, weights = weight)) %&gt;% ggraph(&#39;fr&#39;) + geom_edge_link(alpha = .1) + geom_node_point(aes(size = degree)) Conclusions A co-authorship network like this can get very dense very quickly, because there are many books, and only a limited number of individuals, so there will be many connections between them. It may be more meaningful to filter the data, for example using the edge weight column, to only consider ‘stronger’ relationships in the network (nodes which share several books together). The network also has a large number of ‘isolates’: nodes which are disconnected completely from the full network. These could also be removed, for visual clarity at least. To do this, you could filter to remove nodes with a total degree of one. This projected network is inherently undirected, because the edge ‘shares a book title’ doesn’t have any direction associated with it. Exercises: Construct two spotify ego networks. Write up the clusters found in them. Compare their global network metrics. What does this tell you about the difference between them - for instance how wide or narrow their reach is? "],["week-6-class-1-asking-and-answering-questions-with-networks.html", "Week 6, Class 1: Asking and Answering Questions with Networks What makes a good question of a network? Examples Using the data model Using visualisations Network statistics", " Week 6, Class 1: Asking and Answering Questions with Networks What makes a good question of a network? Because any network is only a model, it is probably best to speak of it being one of many. Even if you use all the data you have, it’s only one possible way of modelling that set of data. You’ve already subsetted from the full network for the twitter data for example. This also means you can create your own networks from your data, using subsets. For example maybe you want to look at a network of only a particular genre of books, or a particular type of person or hashtag in the Twitter network. Difference between descriptive and analytic network analysis. Not that the latter is wrong but should be differentiated. Examples This is divided into two sections: first, we’ll walk through some examples of existing projects, with the aim of explaining how they use network methods to make claims or findings. Hopefully these will inspire you with your own network datasets. Existing examples Ahnerts Tudor project John Ladd - dedications Ingeborg van Vugt - multi-layer networks Michael Gavin, book network Practical Example: ESTC Question: what do publishers and printers cluster by? Looking for groups within ESTC data Run community detection Creat visualisation of results With results, Create report of communities plus members, chart communities and membership over time. For each community, we list the count of book attributes for its members: publication years, genres, publication places, format, book type. Visualise these elements Analyse the results What is the makeup of each group? What findings do we make out of this? Practical example: Determining social media influence using PageRank What is the page rank algorithm? Using the data model filtering your data using a pipeline Pick a subset of your data - filtered by time or geography, or some other facet of the nodes? Getting network statistics before and after filtering. Using visualisations Faceting by some variable - drawing different networks. (maybe using cowplot/patchwork) Network statistics Getting statistics, turning into a tibble, ordering by different measurements Getting and reporting basic global network metrics Networks and regression Statistical approaches to networks - ERGM and SOAM (RSiena) "],["week-6-class-2-reflections-and-pitfalls-of-networks.html", "Week 6, Class 2: Reflections and Pitfalls of Networks What are implications of networks? Network representations Problems with network representation Visualisations", " Week 6, Class 2: Reflections and Pitfalls of Networks What are implications of networks? Hierarchies ‘Rich get richer’ effect Network representations How do we communicate that this is one representation/model of data Problems with network representation Pitfalls etc. Missing data Specific problems with networks and cultural/humanities data Visualisations “There is a tendency when using graphs to become smitten with one’s own data. Even though a graph of a few hundred nodes quickly becomes unreadable, it is often satisfying for the creator because the resulting figure is elegant and complex and may be subjectively beautiful, and the notion that the creator’s data is”complex” fits just fine with the creator’s own interpretation of it. Graphs have a tendency of making a data set look sophisticated and important, without having solved the problem of enlightening the viewer.” —Ben Fry, Visualizing Data: Exploring and Explaining Data with the Processing Environment (Sebastopol, CA: O’Reilly Media, 2007) Do you agree with this statement? Another viewpoint is here: https://gephi.wordpress.com/2011/10/12/everything-looks-like-a-graph-but-almost-nothing-should-ever-be-drawn-as-one/ "],["week-7-final-project.html", "Week 7: Final Project What should the final project look like? R Markdown Final Project Datasets", " Week 7: Final Project There are two stages to the final project: In the final week of the course, either December 14 or 16, you’ll be assigned a 10-minute presentation slot, where you should outline your progress so far, including your research question, datasets used, any problems you encountered (or think you might). This is a chance to get feedback from your peers and course leaders. The final project itself is due on Friday, December 23. What should the final project look like? The final project tasks you with using your network data and related data model to tackle an interesting research questions. The project should take the form of an R Markdown notebook, as you have learned to create in the first few weeks of the course. An R Markdown document is a format which allows you to combine text, chunks of code, and the output of those chunks. You’ll write up this document and then turn it into a HTML page - a process known as ‘knitting’. Practical steps Gather your datasets and make sure they are available. If you use a ‘non standard’ dataset, export a copy of the final dataset used and include it along with the knitted HTML file and the original R Markdown. Project Aim The aim is to communicate to a reader the value of your project and its implications. To do this, the project should take the form of ‘literate programming’, meaning that it should contain code, visualisations, derived data, and text - it should read not just as a piece of code and its output, but as a report and discussion on your findings. Interpretation and discussion is essential. You can also include supplementary elements, for example data. Include the original markdown, all data used, and the knitted html version. Some examples of literate programming final projects R Markdown We have already gone through R Markdown in detail, but it has a number of other features/packages you can use if you want to extend your final project. The packages VisNetwork and Plotly allow you to include interactive visualisations within the report. The DT package allows you to do the same for tables. These elements are not essential, but can help to communicate your results to the reader in some cases. Reproducible research The point of this is that it allows anyone to reproduce your results. This is done by including your code and data within the report. This is done automatically if you use R Markdown, because the code is included by default in your outut. You can use external sources and code/applications, but you should explain what you have used in detail, so that others can follow the steps you’ve carried out and (hopefully) reach the same conclusions. Questions Your final project should be structured to answer the following questions: Introduction - what is your question/problem you’re trying to solve? Datasets - what datasets are you using? Where did you get them? Is it openly-available? Do they have missing data? Did you need to do pre-processing such as data harmonisation? What is your data model? How does your data relate to itself? Method - How did you create the network? What subset of your data did you use and why did you choose it? What additional information about your network (attributes) did you use? What are the basic network statistics of this network? With your ‘finished’ network, how many nodes and edges are there? What is the density? Is it clustered or very dense? Outline the particular network methods you used - community detection, centrality, global network metrics, others? Findings. What are your conclusions? What has the data and network told you? Discussion. What are the implications for this research/method? What does it tell us? What didn’t work? What are the pitfalls of looking at your data like this? What else did you try that didn’t work? Were you able to use visualisations successfully? Final Project Datasets As well as the ESTC and Twitter datasets we have provided for the final project, you can use any network dataset you like. In every case, you should reflect critically on the dataset used, provide it in full for reproducibility, and make a note on what bias it might have and how this might affect your findings. Almost any humanities data can be represented as a network - particularly bipartite. You should outline the dataset you intend on using during the final project presentation. Many of these datasets will require the extraction of structured data or cleaning before they are suitable for network analysis, so bear in mind the time and expertise that will take. Available datasets Below are some other datasets or collections of datasets which might be useful. Important to note: don’t just pick a dataset because you think it might make an interesting network graph. Think of the research question you would like to tackle first. Dataset collections: Stanford Large Network Dataset Collection UCIrvine Network Data Repository Correspondence data: Willem I van Oranje-Nassau: letters and network in 1572 Correspondence of Daniel van der Meulen (1554 - 1600) There is also another dataset of early modern correspondence available, from the State Papers - ask Yann for more details. Citation/Co-occurrence networks Citation network on Kaggle Marvel Universe Social Network (a good example of a bimodal network, that should generally be projected to a unimodal one for analysis) Game of Thrones Books Character Network (and series) Other cultural data: Project Gutenberg (for full texts) RECIRC Project(database of texts by female authors - individual searches can be exported as a .csv file for use in network analysis) Spotify API (as we used in a previous class) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
